{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Eq_QfGIGXC_"
      },
      "source": [
        "### **Búsqueda y Minería de Información 2021-22**\n",
        "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
        "### Grado en Ingeniería Informática, 4º curso\n",
        "# **Motores de búsqueda e indexación**\n",
        "\n",
        "Fechas:\n",
        "\n",
        "* Comienzo: lunes 21 / martes 22 de febrero\n",
        "* Entrega: lunes 28 / martes 29 de marzo (14:00)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYT0Qlrnoy7l"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDFY_K6_pA_J"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "Los objetivos de esta práctica son:\n",
        "\n",
        "* La implementación eficiente de funciones de ránking, particularizada en el modelo vectorial.\n",
        "*\tLa implementación de índices eficientes para motores de búsqueda. \n",
        "*\tLa implementación de un método de búsqueda proximal.\n",
        "*\tLa dotación de estructuras de índice posicional que soporten la búsqueda proximal.\n",
        "*\tLa implementación del algoritmo PageRank.\n",
        "\n",
        "Se desarrollarán implementaciones de índices utilizando un diccionario y listas de postings. Y se implementará el modelo vectorial utilizando estas estructuras más eficientes para la ejecución de consultas.\n",
        "\n",
        "Los ejercicios básicos consistirán en la implementación de algoritmos y técnicas estudiados en las clases de teoría, con algunas propuestas de extensión opcionales. Se podrá comparar el rendimiento de las diferentes versiones de índices y buscadores, contrastando la coherencia con los planteamientos estudiados a nivel teórico.\n",
        "\n",
        "Mediante el nivel de abstracción seguido, se conseguirán versiones intercambiables de índices y buscadores. El único buscador que no será intercambiable es el de Whoosh, que sólo funcionará con sus propios índices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPjq_DVVpDEL"
      },
      "source": [
        "## Material proporcionado\n",
        "\n",
        "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
        "\n",
        "*\tVarias clases e interfaces Python a lo largo de este *notebook*, con las que el estudiante integrará las suyas propias. \n",
        "Las clases parten del código de la práctica anterior.\n",
        "Igual que en la práctica 1, la función **main** implementa un programa que deberá funcionar con las clases a implementar por el estudiante.\n",
        "*\tLas colecciones de prueba de la práctica 1: <ins>toys.zip</ins> (que se descomprime en dos carpetas toy1 y toy2), <ins>docs1k.zip</ins> con 1.000 documentos HTML y un pequeño fichero <ins>urls.txt</ins>. \n",
        "*\tUna colección más grande: <ins>docs10k.zip</ins> con 10.000 documentos HTML.\n",
        "*\tVarios grafos para probar PageRank: <ins>graphs.zip</ins>.\n",
        "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la función main."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAKBQZLLqVXR"
      },
      "outputs": [],
      "source": [
        "import os, os.path\n",
        "import re\n",
        "import math\n",
        "import pickle\n",
        "import zipfile\n",
        "from abc import ABC, abstractmethod\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class Config(object):\n",
        "  # variables de clase\n",
        "  NORMS_FILE = \"docnorms.dat\"\n",
        "  PATHS_FILE = \"docpaths.dat\"\n",
        "  INDEX_FILE = \"serialindex.dat\"\n",
        "  DICTIONARY_FILE = \"dictionary.dat\"\n",
        "  POSTINGS_FILE = \"postings.dat\"\n",
        "\n",
        "class BasicParser:\n",
        "    @staticmethod\n",
        "    def parse(text):\n",
        "        return re.findall(r\"[^\\W\\d_]+|\\d+\", text.lower())\n",
        "\n",
        "def tf(freq):\n",
        "    return 1 + math.log2(freq) if freq > 0 else 0\n",
        "\n",
        "def idf(df, n):\n",
        "    return math.log2((n + 1) / (df + 0.5))\n",
        "\n",
        "\"\"\"\n",
        "    This is an abstract class for the search engines\n",
        "\"\"\"\n",
        "class Searcher(ABC):\n",
        "    def __init__(self, index, parser=BasicParser()):\n",
        "        self.index = index\n",
        "        self.parser = parser\n",
        "    @abstractmethod\n",
        "    def search(self, query, cutoff):\n",
        "        \"\"\" Returns a list of documents encapsulated in a SearchRanking class \"\"\"\n",
        "\n",
        "class Index:\n",
        "    def __init__(self, dir=None):\n",
        "        self.docmap = []\n",
        "        self.modulemap = {}\n",
        "        if dir: self.open(dir)\n",
        "\n",
        "    def add_doc(self, path):\n",
        "        self.docmap.append(path)  # Assumed to come in order\n",
        "\n",
        "    def doc_path(self, docid):\n",
        "        return self.docmap[docid]\n",
        "\n",
        "    def doc_module(self, docid):\n",
        "        if docid in self.modulemap:\n",
        "            return self.modulemap[docid]\n",
        "        return None\n",
        "\n",
        "    def ndocs(self):\n",
        "        return len(self.docmap)\n",
        "\n",
        "    def doc_freq(self, term):\n",
        "        return len(self.postings(term))\n",
        "\n",
        "    def term_freq(self, term, docID):\n",
        "        post = self.postings(term)\n",
        "        if post is None: return 0\n",
        "        for posting in post:\n",
        "            if posting[0] == docID:\n",
        "                return posting[1]\n",
        "        return 0\n",
        "\n",
        "    def total_freq(self, term):\n",
        "        freq = 0\n",
        "        for posting in self.postings(term):\n",
        "            freq += posting[1]\n",
        "        return freq\n",
        "\n",
        "    def postings(self, term):\n",
        "        # used in more efficient implementations\n",
        "        return list()\n",
        "\n",
        "    def positional_postings(self, term):\n",
        "        # used in positional implementations\n",
        "        return list()\n",
        "\n",
        "    def all_terms(self):\n",
        "        return list()\n",
        "\n",
        "    def save(self, dir_path):\n",
        "        if not self.modulemap: self.compute_modules()\n",
        "        p = os.path.join(dir_path, Config.NORMS_FILE)\n",
        "        with open(p, 'wb') as f:\n",
        "            pickle.dump(self.modulemap, f)        \n",
        "\n",
        "    def open(self, dir_path):\n",
        "        try:\n",
        "            p = os.path.join(dir_path, Config.NORMS_FILE)\n",
        "            with open(p, 'rb') as f:\n",
        "                self.modulemap = pickle.load(f)\n",
        "        except OSError:\n",
        "            # the file may not exist the first time\n",
        "            pass\n",
        "\n",
        "    def compute_modules(self):\n",
        "        for term in self.all_terms():\n",
        "            idf_score = idf(self.doc_freq(term), self.ndocs())\n",
        "            post = self.postings(term)\n",
        "            if post is None: continue\n",
        "            for docid, freq in post:\n",
        "                if docid not in self.modulemap: self.modulemap[docid] = 0\n",
        "                self.modulemap[docid] += math.pow(tf(freq) * idf_score, 2)\n",
        "        for docid in range(self.ndocs()):\n",
        "            self.modulemap[docid] = math.sqrt(self.modulemap[docid]) if docid in self.modulemap else 0\n",
        "\n",
        "class Builder:\n",
        "\n",
        "    def __init__(self, dir_path, parser=BasicParser()):\n",
        "        if os.path.exists(dir_path): shutil.rmtree(dir_path)\n",
        "        os.makedirs(dir_path)\n",
        "        self.parser = parser\n",
        "\n",
        "    def build(self, path):\n",
        "        if zipfile.is_zipfile(path):\n",
        "            self.index_zip(path)\n",
        "        elif os.path.isdir(path):\n",
        "            self.index_dir(path)\n",
        "        else:\n",
        "            self.index_url_file(path)\n",
        "\n",
        "    def index_zip(self, filename):\n",
        "        file = zipfile.ZipFile(filename, mode='r', compression=zipfile.ZIP_DEFLATED)\n",
        "        for name in sorted(file.namelist()):\n",
        "            with file.open(name, \"r\", force_zip64=True) as f:\n",
        "                self.index_document(name, BeautifulSoup(f.read().decode(\"utf-8\"), \"html.parser\").text)\n",
        "        file.close()\n",
        "\n",
        "    def index_dir(self, dir_path):\n",
        "        for subdir, dirs, files in os.walk(dir_path):\n",
        "            for file in sorted(files):\n",
        "                path = os.path.join(dir_path, file)\n",
        "                with open(path, \"r\") as f:\n",
        "                    self.index_document(path, f.read())\n",
        "\n",
        "    def index_url_file(self, file):\n",
        "        with open(file, \"r\") as f:\n",
        "            self.index_urls(line.rstrip('\\n') for line in f)\n",
        "\n",
        "    def index_urls(self, urls):\n",
        "        for url in urls:\n",
        "            self.index_document(url, BeautifulSoup(urlopen(url).read().decode(\"utf-8\"), \"html.parser\").text)\n",
        "    \n",
        "    def index_document(self, path, text):\n",
        "        pass\n",
        "    \n",
        "    def commit(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMoae4N7y38C"
      },
      "outputs": [],
      "source": [
        "# from previous lab\n",
        "class SlowVSMSearcher(Searcher):\n",
        "    \n",
        "    def __init__(self, index, parser=BasicParser()):\n",
        "        super().__init__(index, parser)\n",
        "\n",
        "    def search(self, query, cutoff):\n",
        "        qterms = self.parser.parse(query)\n",
        "        ranking = SearchRanking(cutoff)\n",
        "        for docid in range(self.index.ndocs()):\n",
        "            score = self.score(docid, qterms)\n",
        "            if score:\n",
        "                ranking.push(self.index.doc_path(docid), score)\n",
        "        return ranking\n",
        "\n",
        "    def score(self, docid, qterms):\n",
        "        prod = 0\n",
        "        for term in qterms:\n",
        "            prod += tf(self.index.term_freq(term, docid)) \\\n",
        "                    * idf(self.index.doc_freq(term), self.index.ndocs())\n",
        "        mod = self.index.doc_module(docid)\n",
        "        if mod:\n",
        "            return prod / mod\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-7gj9Rxx6LD"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import whoosh\n",
        "except ModuleNotFoundError:\n",
        "  !pip install whoosh\n",
        "  import whoosh\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "from whoosh.formats import Format\n",
        "from whoosh.qparser import QueryParser\n",
        "\n",
        "# A schema in Whoosh is the set of possible fields in a document in\n",
        "# the search space. We just define a simple 'Document' schema, with\n",
        "# a path (a URL or local pathname) and a content.\n",
        "SimpleDocument = Schema(\n",
        "        path=ID(stored=True),\n",
        "        content=TEXT(phrase=False))\n",
        "ForwardDocument = Schema(\n",
        "        path=ID(stored=True),\n",
        "        content=TEXT(phrase=False,vector=Format))\n",
        "PositionalDocument = Schema(\n",
        "        path=ID(stored=True),\n",
        "        content=TEXT(phrase=True))\n",
        "\n",
        "class WhooshBuilder(Builder):\n",
        "    def __init__(self, dir, schema=SimpleDocument):\n",
        "        super().__init__(dir)\n",
        "        self.whoosh_writer = whoosh.index.create_in(dir, schema).writer(procs=1, limitmb=16384, multisegment=True)\n",
        "        self.dir = dir\n",
        "\n",
        "    def index_document(self, p, text):\n",
        "        self.whoosh_writer.add_document(path=p, content=text)\n",
        "\n",
        "    def commit(self):\n",
        "        self.whoosh_writer.commit()\n",
        "        index = WhooshIndex(self.dir)\n",
        "        index.save(self.dir)\n",
        "\n",
        "class WhooshForwardBuilder(WhooshBuilder):\n",
        "    def __init__(self, dir_path):\n",
        "        super().__init__(dir_path, ForwardDocument)\n",
        "\n",
        "    def commit(self):\n",
        "        self.whoosh_writer.commit()\n",
        "        index = WhooshForwardIndex(self.dir)\n",
        "        index.save(self.dir)\n",
        "\n",
        "class WhooshPositionalBuilder(WhooshBuilder):\n",
        "    def __init__(self, dir_path):\n",
        "        super().__init__(dir_path, PositionalDocument)\n",
        "\n",
        "    def commit(self):\n",
        "        self.whoosh_writer.commit()\n",
        "        index = WhooshPositionalIndex(self.dir)\n",
        "        index.save(self.dir)\n",
        "\n",
        "class WhooshIndex(Index):\n",
        "    def __init__(self, dir):\n",
        "        super().__init__(dir)\n",
        "        self.whoosh_reader = whoosh.index.open_dir(dir).reader()    \n",
        "\n",
        "    def total_freq(self, term):\n",
        "        return self.whoosh_reader.frequency(\"content\", term)\n",
        "\n",
        "    def doc_freq(self, term):\n",
        "        return self.whoosh_reader.doc_frequency(\"content\", term)\n",
        "\n",
        "    def doc_path(self, docid):\n",
        "        return self.whoosh_reader.stored_fields(docid)['path']\n",
        "\n",
        "    def ndocs(self):\n",
        "        return self.whoosh_reader.doc_count()\n",
        "\n",
        "    def all_terms(self):\n",
        "        return list(self.whoosh_reader.field_terms(\"content\"))\n",
        "\n",
        "    def postings(self, term):\n",
        "        return self.whoosh_reader.postings(\"content\", term).items_as(\"frequency\") \\\n",
        "            if self.doc_freq(term) > 0 else []\n",
        "\n",
        "class WhooshForwardIndex(WhooshIndex):\n",
        "    def term_freq(self, term, docID) -> int:\n",
        "        if self.whoosh_reader.has_vector(docID, \"content\"):\n",
        "            v = self.whoosh_reader.vector(docID, \"content\")\n",
        "            v.skip_to(term)\n",
        "            if v.id() == term:\n",
        "                return v.value_as(\"frequency\")\n",
        "        return 0\n",
        "\n",
        "class WhooshPositionalIndex(WhooshIndex):\n",
        "    def positional_postings(self, term):\n",
        "        return self.whoosh_reader.postings(\"content\", term).items_as(\"positions\") \\\n",
        "            if self.doc_freq(term) > 0 else []\n",
        "\n",
        "class WhooshSearcher(Searcher):\n",
        "    def __init__(self, dir):\n",
        "        self.whoosh_index = whoosh.index.open_dir(dir)\n",
        "        self.whoosh_searcher = self.whoosh_index.searcher()\n",
        "        self.qparser = QueryParser(\"content\", schema=self.whoosh_index.schema)\n",
        "\n",
        "    def search(self, query, cutoff):\n",
        "        return map(lambda scoredoc: (self.doc_path(scoredoc[0]), scoredoc[1]),\n",
        "                   self.whoosh_searcher.search(self.qparser.parse(query), limit=cutoff).items())\n",
        "\n",
        "    def doc_path(self, docid):\n",
        "        return self.whoosh_index.reader().stored_fields(docid)['path']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKBXprvhpqQr"
      },
      "source": [
        "## Calificación\n",
        "\n",
        "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado. No obstante, aquellos ejercicios marcados con un asterisco (*) tienen una complejidad un poco superior a los demás (que suman 7.5 puntos), y permiten, si se realizan todos, una nota superior a 10. \n",
        "\n",
        "El peso de la nota de esta práctica en la calificación final de prácticas es del **40%**.\n",
        "\n",
        "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. La calidad se valorará por los **resultados** conseguidos (economía de consumo de RAM, disco y tiempo; tamaño de las colecciones que se consigan indexar) pero también del **mérito** en términos del interés de las técnicas aplicadas y la buena programación.\n",
        "\n",
        "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
        "\n",
        "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la función *main* así como otros main de prueba adicionales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfMu2CuGprtk"
      },
      "source": [
        "## Entrega\n",
        "\n",
        "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--pSxv2vpvUg"
      },
      "source": [
        "## Indicaciones\n",
        "\n",
        "Se sugiere trabajar en la práctica de manera incremental, asegurando la implementación de soluciones sencillas y mejorándolas de forma modular (la propia estructura de ejercicios plantea ya esta forma de trabajar).\n",
        "\n",
        "Se podrán definir clases o módulos adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: la función **main** deberá ejecutar correctamente <ins>sin ninguna modificación</ins> (más allá de comentar aquellos ejercicios que no se hayan realizado).\n",
        "\n",
        "Asimismo, se recomienda indexar sin ningún tipo de stopwords ni stemming, para poder hacer pruebas más fácilmente con ejemplos “de juguete”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3jRLNZmpEk_"
      },
      "source": [
        "# Ejercicio 1: Implementación de un modelo vectorial eficiente\n",
        "\n",
        "Se mejorará la implementación de la práctica anterior aplicando algoritmos estudiados en las clases de teoría. En particular, se utilizarán listas de postings en lugar de un índice forward.\n",
        "\n",
        "La reimplementación seguirá haciendo uso de la clase abstracta Index, y se podrá probar con cualquier implementación de esta clase (tanto la implementación de índice sobre Whoosh como las propias). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be3vDQNxdWbo"
      },
      "source": [
        "## Ejercicio 1.1: Método orientado a términos (3pt)\n",
        "\n",
        "Escribir una clase TermBasedVSMSearcher que implemente el modelo vectorial coseno por el método orientado a términos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ppr9PtZmduql"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "from collections import OrderedDict\n",
        "import heapq\n",
        "\n",
        "class TermBasedVSMSearcher(Searcher):\n",
        "    # Your new code here (exercise 1.1) #\n",
        "    def __init__(self, index, parser=BasicParser()):\n",
        "        super().__init__(index, parser)\n",
        "\n",
        "    def search(self, query, cutoff):\n",
        "        q_terms = self.parser.parse(query)\n",
        "        ranking = SearchRanking(cutoff)\n",
        "        posting_vals: Dict = {}\n",
        "\n",
        "        for term in q_terms:\n",
        "            for docid, _ in self.index.postings(term):\n",
        "                if docid not in posting_vals.keys():\n",
        "                    posting_vals[docid] = []\n",
        "                posting_vals[docid].append(term)\n",
        "\n",
        "        for docid in posting_vals.keys():\n",
        "            score = self.score(docid, posting_vals[docid])\n",
        "            if score:\n",
        "                ranking.push(self.index.doc_path(docid), score)\n",
        "\n",
        "        return ranking\n",
        "\n",
        "    def score(self, docid, q_terms):\n",
        "        prod = 0\n",
        "        for term in q_terms:\n",
        "            prod += tf(self.index.term_freq(term, docid)) * idf(self.index.doc_freq(term), self.index.ndocs())\n",
        "\n",
        "        mod = self.index.doc_module(docid)\n",
        "        if mod:\n",
        "            return prod / mod\n",
        "         \n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3YGEGm7haop"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "Al ser una búsqueda orientada a términos, iteraremos primero sobre los términos, de tal forma que para cada término que buscamos, recorreremos todos los documentos, asociando el docid de cada documento.\n",
        "\n",
        "Para calcular el score, obtendremos resultados parciales del producto tf*idf, esto quiere decir que cada término tendrá varios resultados, teniendo que sumarlos sucesivamente para calcular el score final (ya que hemos iterado sobre términos primero, habrá que hacer lo mismo al calcular el score). Finalmente se calcula el módulo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ti8qGedgNB"
      },
      "source": [
        "## Ejercicio 1.2: Método orientado a documentos* (1pt)\n",
        "\n",
        "Implementar el método orientado a documentos (con heap de postings) en una clase DocBasedVSMSearcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzZ-6OG0dvwX"
      },
      "outputs": [],
      "source": [
        "class DocBasedVSMSearcher(Searcher):\n",
        "    def __init__(self, index, parser=BasicParser()):\n",
        "        super().__init__(index, parser)\n",
        "\n",
        "    def search(self, query, cutoff):\n",
        "        q_terms = self.parser.parse(query)\n",
        "        posting_list: List = []\n",
        "        ndocs = self.index.ndocs()\n",
        "        rank = SearchRanking(cutoff)\n",
        "        score = [0] * ndocs\n",
        "        index = [0] * len(q_terms)\n",
        "\n",
        "        # Recuperar lista de postings # \n",
        "        for q in q_terms:\n",
        "            posting_list += [[p for p in self.index.postings(q)]]\n",
        "        \n",
        "        heap = []\n",
        "        heapq.heapify(heap)\n",
        "        #Tras crear un min heap, iteramos consecuencialmente en orden de docID \n",
        "        for q in range(len(q_terms)):\n",
        "            heapq.heappush(heap, (posting_list[q][index[q]], q))\n",
        "            index[q] += 1 \n",
        "        \n",
        "        docActual = heapq.nsmallest(1, heap)[0][0][0]\n",
        "        try:\n",
        "            # Obtenemos elemento, docID, frecuencia y el score (que sumaremos en el caso de haber más en el mismo docID)\n",
        "            while(1):\n",
        "                element = heapq.heappop(heap)\n",
        "                docID = element[0][0]\n",
        "                freq = element[0][1]\n",
        "                q = element[1]\n",
        "                partial_score = tf(freq) * idf(self.index.doc_freq(q_terms[q]), ndocs)\n",
        "\n",
        "                # Cuando cambiamos de docID, sumaremos todos los scores parciales #\n",
        "                if docID != docActual:\n",
        "                    score[docActual] /= self.index.doc_module(docActual)\n",
        "                    # Tras hacer la operación del módulo, si el heap está incompleto se añade directamente el docID y su score\n",
        "                    if len(rank.ranking) < rank.cutoff:\n",
        "                        rank.push(self.index.doc_path(\n",
        "                            docActual), rank[docActual])\n",
        "                    # Si el heap está completo, entonces hay que ver si el score total es mayor que el menor del heap (ya que es un min heap)\n",
        "                    elif score > heapq.nsmallest(1, rank.ranking)[0][0]:\n",
        "                        rank.push(self.index.doc_path(\n",
        "                            docActual), rank[docActual])\n",
        "\n",
        "                    docActual = docID\n",
        "                rank[docActual] += partial_score\n",
        "\n",
        "                # Si no se llega al número de índices de la lista de posting, entonces se añade al heap de Ranking\n",
        "                if index[q] < len(posting_list[q]):\n",
        "                    heapq.heappush(heap, (posting_list[q][index[q]], q))\n",
        "                    index[q] += 1\n",
        "\n",
        "        # El heap está vacío\n",
        "        except Exception:  \n",
        "            score[docActual] /= self.index.doc_module(docActual)\n",
        "            if len(rank.ranking) < rank.cutoff:\n",
        "                rank.push(self.index.doc_path(docActual), score[docActual])\n",
        "            elif score > heapq.nsmallest(1, rank.ranking)[0][0]:\n",
        "                rank.push(self.index.doc_path(docActual), score[docActual])\n",
        "\n",
        "        return rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xYd4hzhukr"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "Este método está orientado a documentos, y por lo tanto la lista de postings estará ordenado por docIDs. Construiremos un min heap (del tamaño de la query) y tras iterar consecuencialmente en función del docID, llenaremos el heap.\n",
        "\n",
        "A continuación, obtenemos el docID más pequeño y cogemos el elemento, su frecuencia y el tf*idf como score parcial (en función del número de veces que esté ese docID tendremos más scores parciales). Una vez que hemos vaciado el heap de un docID, dividiremos el sumatorio de los scores parciales entre el módulo y lo meteremos al min heap si ese score final es mayor que el menor score del heap.\n",
        "\n",
        "Finalmente, dará una excepción en el caso de que el heap esté vacío y se devuelve el heap ranking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpXHr18Cdl2Q"
      },
      "source": [
        "## Ejercicio 1.3: Heap de ránking (0.5pt)\n",
        "\n",
        "Reimplementar la clase entregada SearchRanking para utilizar un heap de ránking (se recomienda usar el módulo [heapq](https://docs.python.org/3/library/heapq.html)). Nótese que esta opción se aprovecha mejor con la implementación orientada a documentos, aunque es compatible con la orientada a términos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOfT2yZGpMNi"
      },
      "outputs": [],
      "source": [
        "class SearchRanking:\n",
        "    # TODO: to be implemented as heap (exercise 1.3) #\n",
        "    def __init__(self, cutoff):\n",
        "        self.ranking: List = [] # implementation as list, not as heap! TO BE MODIFIED\n",
        "        heapq.heapify(self.ranking)\n",
        "        self.cutoff = cutoff\n",
        "\n",
        "    def push(self, docid, score):\n",
        "        #self.ranking.append((docid, score))\n",
        "        heapq.heappush(self.ranking, (score,docid))\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter([tuple(reversed(e)) for e in heapq.nlargest(min(len(self.ranking), self.cutoff), self.ranking)])\n",
        "        min_l = min(len(self.ranking), self.cutoff)\n",
        "        ## sort ranking\n",
        "        self.ranking.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        return iter(self.ranking[0:min_l])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJDzjUp-hwNZ"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "La clase SearchRanking tiene que ser reimplementada, dejando de ser una lista para ser un heap. Esto lo hacemos gracias a la librería heapq con la función \"heapify\", que convierte una lista en un MinHeap. El cutoff se mantiene solo que ahora se hará en el heap en vez de en la lista.\n",
        "\n",
        "Para pushear, simplemente utilizamos la función \"heappush\" dada por heapq, y pusheamos una tupla que guarda el score y el id del documento. Que push sea de esta forma (score,docid) se debe a que luego, para iterar, lo haremos en función del score, ya que queremos que esté ordenado por la puntuación que tiene antes que por el docid (ya que además un término puede aparecer en varios documentos).\n",
        "\n",
        "Finalmente se hace el ranking teniendo en cuenta el cutoff previamente especificado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNkPcUjMpNRn"
      },
      "source": [
        "# Ejercicio 2: Índice en RAM (3pt)\n",
        "\n",
        "Implementar un índice propio que pueda hacer las mismas funciones que la implementación basada en Whoosh definida en la práctica 1. Como primera fase más sencilla, los índices se crearán completamente en RAM. Se guardarán a disco y leerán de disco en modo serializado (ver módulo [pickle](https://docs.python.org/3/library/pickle.html)).\n",
        "\n",
        "Para guardar el índice se utilizarán los nombres de fichero definidos por las variables estáticas de la clase Config. \n",
        "\n",
        "Antes de guardar el índice, se borrarán todos los ficheros que pueda haber creados en el directorio del índice. Asimismo, el directorio se creará si no estuviera creado, de forma que no haga falta crearlo a mano. Este detalle se hará igual en los siguientes ejercicios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVzsIg0Zev7a"
      },
      "source": [
        "## Ejercicio 2.1: Estructura de índice\n",
        "\n",
        "Implementar la clase RAMIndex como subclase de Index con las estructuras necesarias: diccionario, listas de postings, más la información que se necesite. \n",
        "\n",
        "Para este ejercicio en las listas de postings sólo será necesario guardar los docIDs y las frecuencias; no es necesario almacenar las posiciones de los términos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqSKneeSe2bN"
      },
      "outputs": [],
      "source": [
        "class RAMIndex(Index):\n",
        "    def __init__(self, dir_path=None):\n",
        "        self.terms_dict = {}\n",
        "        super().__init__(dir_path)\n",
        "\n",
        "    def postings(self, term):\n",
        "        return self.terms_dict[term]\n",
        "\n",
        "    def all_terms(self):\n",
        "        return self.terms_dict.keys()\n",
        "\n",
        "    def save(self, dir_path):\n",
        "        f = open(os.path.join(dir_path, Config.PATHS_FILE), 'wb')\n",
        "        pickle.dump(self.docmap, f)\n",
        "        f.close()\n",
        "        f = open(os.path.join(dir_path, Config.DICTIONARY_FILE), 'wb')\n",
        "        self.terms_dict = OrderedDict(sorted(self.terms_dict.items()))\n",
        "        pickle.dump(self.terms_dict, f)\n",
        "        f.close()\n",
        "        super().save(dir_path)\n",
        "\n",
        "    def open(self, dir_path):\n",
        "        super().open(dir_path)\n",
        "        try:\n",
        "            f = open(os.path.join(dir_path, Config.PATHS_FILE), 'rb')\n",
        "            self.docmap = pickle.load(f)\n",
        "            f.close()\n",
        "            f = open(os.path.join(dir_path, Config.DICTIONARY_FILE), 'rb')\n",
        "            self.terms_dict = pickle.load(f)\n",
        "            f.close()\n",
        "        except OSError:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucORmwfCh4Um"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "Al guardar los términos y los postings en memoria, solo se inicializa un diccionario (que al principio está vacío). Para obtener los postings de un término solo hay que buscar en el diccionario el término que nos interesa y, para obtener todos los términos del índice se devuelven las claves del diccionario (que son, de hecho, los términos).\n",
        "\n",
        "Para guardar, mediante la librería pickle, podemos meter todo el diccionario en un fichero (DICTIONARY_FILE) con la función \"dump\", por lo tanto todos los datos del diccionario están condensados en el fichero de esta forma es más fácil cargar los datos con la función \"load\", ya que simplemente obtenemos todos los datos del diccionario que hemos guardado en el fichero anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbc9ng8e28p"
      },
      "source": [
        "## Ejercicio 2.2 Construcción del índice\n",
        "\n",
        "Implementar la clase RAMIndexBuilder como subclase de Builder, que cree todo el índice en RAM a partir de una colección de documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHQ4UCf5pTw8"
      },
      "outputs": [],
      "source": [
        "class RAMIndexBuilder(Builder):\n",
        "    # Your new code here (exercise 2.2) #\n",
        "    def __init__(self, dir_path, parser=BasicParser()):\n",
        "        super().__init__(dir_path, parser)\n",
        "        self.path = dir_path\n",
        "        self.index = RAMIndex(dir_path)\n",
        "\n",
        "    def index_document(self, path, text):\n",
        "        doc_id = self.index.ndocs()\n",
        "\n",
        "        self.index.add_doc(path)\n",
        "\n",
        "        text = BasicParser.parse(text)\n",
        "        term_freq: Dict = {}\n",
        "\n",
        "        for term in text:\n",
        "            if self.index.terms_dict.get(term) is None:\n",
        "                term_freq[term] = 0\n",
        "                self.index.terms_dict[term] = []\n",
        "\n",
        "            elif term_freq.get(term) is None:\n",
        "                term_freq[term] = 0\n",
        "\n",
        "            term_freq[term] += 1\n",
        "\n",
        "        for term in term_freq.keys():\n",
        "            self.index.terms_dict[term].append(\n",
        "                [doc_id, term_freq[term]])\n",
        "\n",
        "    def commit(self):\n",
        "        self.index.save(self.path)  \n",
        "        f = open(self.path + Config.INDEX_FILE, \"wb\")\n",
        "        pickle.dump(self.index, f)\n",
        "        f.close()\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_mxRswZh74N"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "Para indexar un término iteraremos sobre el texto de los documentos que hemos recogido previamente, de tal forma que si es la primera vez que nos encontramos un términos lo añadimos a un diccionario donde guardaremos tanto término como frecuencia. Inicializamos el valor de la frecuencia a 0, de tal forma que cada vez que aparezca el término se sumará uno a la frecuencia.\n",
        "\n",
        "Después de comprobar si el término está ya o no en el diccionario y sumar la frecuencia en el caso de que ya estuviera, añadimos al término el id del documento donde aparece y la frecunecia del término, dando lugar a la lista de postings.\n",
        "\n",
        "Al hacer commit, el índice se guarda y se vuelca la información referente al mismo en el fichero INDEX_FILE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lOWgbqZpV01"
      },
      "source": [
        "# Ejercicio 3: Índice en disco* (1pt)\n",
        "\n",
        "Reimplementar los índices definiendo las clases DiskIndex y DiskIndexBuilder de forma que:\n",
        "\n",
        "*\tEl índice se siga creando entero en RAM (por ejemplo, usando estructuras similares a las del ejercicio 2).\n",
        "*\tPero el índice se guarde en disco dato a dato (docIDs, frecuencias, etc.).\n",
        "*\tAl cargar el índice, sólo el diccionario se lee a RAM, y se accede a las listas de postings en disco cuando son necesarias (p.e. en tiempo de consulta).\n",
        "\n",
        "Se sugiere guardar el diccionario en un fichero y las listas de postings en otro, utilizando los nombres de fichero definidos como variables estáticas en la clase Config.\n",
        "\n",
        "Observación: se sugiere inicialmente guardar en disco las estructuras de índice en modo texto para poder depurar los programas. Una vez asegurada la corrección de los programas, puede ser más fácil pasar a modo binario o serializable (usando el módulo pickle como en ejercicios previos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br7yqFrnpZYl"
      },
      "outputs": [],
      "source": [
        "class DiskIndex(Index):\n",
        "    # Your new code here (exercise 3*) #\n",
        "    def __init__(self, dir_path=None):\n",
        "        self.postings_dict: Dict = {}\n",
        "        self.terms_dict: Dict = {}\n",
        "        self.postings_path = os.path.join(dir_path, Config.POSTINGS_FILE)\n",
        "        self.count = 0\n",
        "        super().__init__(dir_path)\n",
        "        \n",
        "    def postings(self, term):\n",
        "        postings_ret: List = []\n",
        "        f = open(self.postings_path, 'r')\n",
        "        f.seek(self.terms_dict[term])\n",
        "\n",
        "        doc_amount, raw_postings = f.readline().strip().split(\"-\")\n",
        "        raw_postings = raw_postings.split(\" \")\n",
        "\n",
        "        for i in range(0, int(doc_amount) * 2, 2):\n",
        "            postings_ret.append([int(raw_postings[i]), int(raw_postings[i+1])])\n",
        "\n",
        "        f.close()\n",
        "        return postings_ret\n",
        "\n",
        "    def all_terms(self):\n",
        "        return self.terms_dict.keys()\n",
        "\n",
        "    def doc_freq(self, term):\n",
        "        f = open(self.postings_path, 'r')\n",
        "        f.seek(self.terms_dict[term])\n",
        "        result = f.readline().split(\"-\")[0]\n",
        "        f.close()\n",
        "        return int(result)\n",
        "\n",
        "    def save(self, dir_path):\n",
        "        f = open(os.path.join(dir_path, Config.PATHS_FILE), 'wb')\n",
        "        pickle.dump(self.docmap, f)\n",
        "        f.close()\n",
        "        \n",
        "        f = open(os.path.join(dir_path, Config.POSTINGS_FILE), 'w')\n",
        "        for term, postings in self.postings_dict.items():\n",
        "                self.terms_dict[term] = f.tell()\n",
        "                n_postings = len(postings)\n",
        "                f.write(str(n_postings) + \"-\")\n",
        "                for p in postings:\n",
        "                    f.write(str(p[0]) + \" \")\n",
        "                    f.write(str(p[1]) + \" \")\n",
        "                f.write(\"\\n\")\n",
        "        f.close()\n",
        "        \n",
        "        f = open(os.path.join(dir_path, Config.DICTIONARY_FILE), 'wb')\n",
        "        self.terms_dict = OrderedDict(sorted(self.terms_dict.items()))\n",
        "        pickle.dump(self.terms_dict, f)\n",
        "        f.close()\n",
        "\n",
        "        super().save(dir_path)\n",
        "\n",
        "\n",
        "    def open(self, dir_path):\n",
        "        super().open(dir_path)\n",
        "        self.postings_dict = {}\n",
        "        try:\n",
        "            f = open(os.path.join(dir_path, Config.PATHS_FILE), 'rb')\n",
        "            self.docmap = pickle.load(f)\n",
        "            f.close()\n",
        "            f = open(os.path.join(dir_path, Config.DICTIONARY_FILE), 'rb')\n",
        "            self.terms_dict = pickle.load(f)\n",
        "            f.close()\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "\n",
        "class DiskIndexBuilder(Builder):\n",
        "    # Your new code here (exercise 3*) #\n",
        "    def __init__(self, dir_path, parser=BasicParser()):\n",
        "        super().__init__(dir_path, parser)\n",
        "        self.path = dir_path\n",
        "        self.index = DiskIndex(dir_path)\n",
        "\n",
        "    def index_document(self, path, text):\n",
        "        doc_id = self.index.ndocs()\n",
        "\n",
        "        self.index.add_doc(path)\n",
        "\n",
        "        text = BasicParser.parse(text)\n",
        "        term_freq: Dict = {}\n",
        "\n",
        "        for term in text:\n",
        "            if self.index.postings_dict.get(term) is None:\n",
        "                term_freq[term] = 0\n",
        "                self.index.postings_dict[term] = []\n",
        "\n",
        "            elif term_freq.get(term) is None:\n",
        "                term_freq[term] = 0\n",
        "\n",
        "            term_freq[term] += 1\n",
        "\n",
        "        for term in term_freq.keys():\n",
        "            self.index.postings_dict[term].append(\n",
        "                [doc_id, term_freq[term]])\n",
        "\n",
        "    def commit(self):\n",
        "        self.index.save(self.path)  \n",
        "        f = open(self.path + Config.INDEX_FILE, \"wb\")\n",
        "        pickle.dump(self.index, f)\n",
        "        f.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzTje0viiM9I"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "3.1 - DiskIndex\n",
        "\n",
        "Empezaremos hablando del planteamiento seguido para el indice, en este caso tenemos mas estructuras que en el RamIndex aunque compartimos el diccionario de terminos. Como novedad vemos la agregacion de un diccionario de postings, otro de terminos y sus offsets para poder acceder a la memoria, otro diccionario con los paths.\n",
        "\n",
        "En este caso podemos ver una funcion de postings algo mas elaborada, creadno una estructura simple a la hora de trabajar y procesar, guardamos la posicion que indique el offset para dicho termino, asi podemos leer directamente la fila del documento cargando los postings.\n",
        "\n",
        "Otra funcion modificada en este caso es la funcion doc_freq con respecto a la clase base de Index. Para no tener que leer la lista de postings cada vez, a la hora de guardar la lista de postings en la funcion save hemos agregado el valor, necesitando asi solo leer el valor del fichero.\n",
        "\n",
        "Y hablando del save, tantno la funcion save como open hacen uso de pickle para el formato del fichero.\n",
        "\n",
        "3.2 - DiskIndexBuilder\n",
        "\n",
        "Al contrario que el DiskIndex que tiene deiferencias a la hora de controlar los datos, el builder es igual al de RAMIndex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcxuclLwpaM-"
      },
      "source": [
        "# Ejercicio 4: Motor de búsqueda proximal* (1pt)\n",
        "\n",
        "Implementar un método de búsqueda proximal en una clase ProximitySearcher, utilizando las interfaces de índices posicionales. Igual que en los ejercicios anteriores, se sugiere definir esta clase como subclase (directa o indirecta) de Searcher. Para empezar a probar este buscador, se proporciona una implementación de indexación posicional basada en Whoosh (WhooshPositionalIndex)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3uq565SpfSA"
      },
      "outputs": [],
      "source": [
        "class ProximitySearcher(Searcher):\n",
        "    # Your new code here (exercise 4*) #\n",
        "    def __init__(self, index, parser=BasicParser()):\n",
        "        super().__init__(index, parser)\n",
        "    \n",
        "    def search(self, query, cutoff):\n",
        "        q_terms = self.parser.parse(query)\n",
        "        rank = SearchRanking(cutoff)\n",
        "        new_doc_ids = set()\n",
        "        final_positions = {}\n",
        "\n",
        "        for doc_id, _ in self.index.postings(q_terms[0]):\n",
        "            new_doc_ids.add(doc_id)\n",
        "\n",
        "        if len(q_terms) > 1:\n",
        "            for term in q_terms[1:]:\n",
        "                old_doc_ids = set()\n",
        "                for doc_id, _ in self.index.postings(term):\n",
        "                    old_doc_ids.add(doc_id)\n",
        "                new_doc_ids &= old_doc_ids\n",
        "\n",
        "        for term in q_terms:\n",
        "            term_clean_positions = {}\n",
        "            term_positions = self.index.positional_postings(term)\n",
        "            for doc_positions in term_positions:\n",
        "                if doc_positions[0] in new_doc_ids:\n",
        "                    term_clean_positions[doc_positions[0]] = doc_positions[1]\n",
        "            final_positions[term] = term_clean_positions\n",
        "\n",
        "        for doc_id in new_doc_ids:\n",
        "            rank.push(self.index.doc_path(doc_id), \n",
        "                         self.score(final_positions, doc_id, q_terms))\n",
        "\n",
        "        return rank\n",
        "\n",
        "    def score(self, final_positions, doc_id, q_terms):\n",
        "        pos_list = []\n",
        "        q_len = len(q_terms)\n",
        "\n",
        "        for term in q_terms: \n",
        "            pos_list.append(final_positions[term][doc_id] + [math.inf])\n",
        "\n",
        "        if q_len == 1:\n",
        "            return float(len(pos_list[0]) - 1)\n",
        "\n",
        "        a = - 1\n",
        "        score = 0\n",
        "        pointer = [0] * q_len\n",
        "        b = max([t_list[0] for t_list in pos_list])\n",
        "\n",
        "        while b != math.inf:\n",
        "            i = 0\n",
        "            for j in range(q_len):\n",
        "                while pos_list[j][pointer[j]+1] <= b:\n",
        "                    pointer[j] += 1\n",
        "                if pos_list[j][pointer[j]] < pos_list[i][pointer[i]]:\n",
        "                    i = j\n",
        "            a = pos_list[i][pointer[i]]\n",
        "            score += 1 / (b - a - q_len + 2)\n",
        "            b = pos_list[i][pointer[i]+1]\n",
        "\n",
        "        return score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d4hWTstiOIT"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "4.1 - ProximitySearcher\n",
        "\n",
        "La implementación de este algoritmo ha presentado un par de problemas, nos hemos basado en ejercicios de teoria y posts de internet.\n",
        "\n",
        "Comenzamos parseando la query y aplicando el cutoff para observar cuales son los documentos validos para la consulta.\n",
        "Cogemos los documentos que contengan el primer termino y aplicamos la interseccion con los que contienen el segundo, la solucion la intersecamos con el tercer termino y asi sucesivamente con todos. Tendremos asi un set con los ids de los documentos que contienen todos los terminos de la consulta, una vez tenemos esto, limpiamos via positional postings. lo que hacemos es basicamente llamar a positional postings eliminando todos los documentos que no ficuren en el set anteriormente sacado, una vez tenemos estopara cada documento llamamos a la funcion score, que obtendrá B y A de cada grupo de elementos y calculará el score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPPWV7pepf85"
      },
      "source": [
        "# Ejercicio 5: Índice posicional* (1pt)\n",
        "\n",
        "Implementar una variante adicional de índice (como subclase si se considera oportuno) que extienda las estructuras de índices con la inclusión de posiciones en las listas de postings. La implementación incluirá una clase PositionalIndexBuilder para la construcción del índice posicional así como una clase PositionalIndex para proporcionar acceso al mismo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg8MIMpipih1"
      },
      "outputs": [],
      "source": [
        "class PositionalIndex(RAMIndex):\n",
        "    # Your new code here (exercise 5*) #\n",
        "    # Note that it may be better to inherit from a different class\n",
        "    # if your index extends a particular type of index\n",
        "    # For example: PositionalIndex(RAMIndex)\n",
        "    def __init__(self, dir_path=None):\n",
        "        self.terms_dict = {}\n",
        "        super().__init__(dir_path)\n",
        "    \n",
        "    def positional_postings(self, term):\n",
        "        dict = self.terms_dict[term]\n",
        "        returned_postings = [] \n",
        "        for doc, pos in dict.items():\n",
        "            returned_postings.append(tuple((doc, pos))) \n",
        "        \n",
        "        return returned_postings\n",
        "\n",
        "    def postings(self, term):\n",
        "        dict = self.terms_dict[term]\n",
        "        returned_postings = []\n",
        "        for doc, pos in dict.items():\n",
        "            returned_postings.append((doc, len(pos)))\n",
        "        return returned_postings\n",
        "\n",
        "\n",
        "class PositionalIndexBuilder(RAMIndexBuilder):\n",
        "    # Your new code here (exercise 5*) #\n",
        "    # Same note as for PositionalIndex\n",
        "    def __init__(self, dir_path, parser=BasicParser()):\n",
        "        super().__init__(dir_path, parser)\n",
        "        self.index = PositionalIndex(dir_path)\n",
        "\n",
        "    def index_document(self, path, text):\n",
        "        doc_id = self.index.ndocs()\n",
        "\n",
        "        self.index.add_doc(path)\n",
        "\n",
        "        text = BasicParser.parse(text)\n",
        "        term_freq: Dict = {}\n",
        "\n",
        "        for pos, term in enumerate(text):\n",
        "            if self.index.terms_dict.get(term) is None:\n",
        "                term_freq[term] = []\n",
        "                self.index.terms_dict[term] = {}\n",
        "\n",
        "            elif term_freq.get(term) is None:\n",
        "                term_freq[term] = []\n",
        "\n",
        "            term_freq[term].append(pos)\n",
        "\n",
        "        for term in term_freq.keys():\n",
        "            self.index.terms_dict[term][doc_id] = term_freq[term]\n",
        "\n",
        "        return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1gukouXiPV3"
      },
      "source": [
        "### Explicación/documentación, indicando además el tipo de índice que se ha implementado y los aspectos que sean destacables\n",
        "\n",
        "5.1 - PositionalIndex\n",
        "\n",
        "Para la implementacion de este ejercicio, hemos hecho que se herede de la clase RAMIndex, y para la implementación simplemente se ha iterado sobre las claves del diccionario y comprobalo la longitud de los postings dando la frecuencia. La lista de postings posicionales se ha obtenido iterando sobre su diccionario y obteninedo la lista de posiciones de cada id de documetno, el resto sigue el mismo flujo que el RAMIndex\n",
        "\n",
        "5.2 - PositionalIndexBuilder\n",
        "\n",
        "Para construir el índice hemos almacenado posiciones, con lo que vamos a tener un índice que sea un diccionario término-diccionario, donde este segundo diccionario será uno con la forma docid-posiciones. Así, el diccionario auxiliar que teníamos antes con la forma término-frecuencia, se convierte en uno de la forma término-posiciones, con lo que podremos conocer las posiciones de manera sencilla."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6HbYGn8pjKZ"
      },
      "source": [
        "# Ejercicio 6: PageRank (1pt)\n",
        "\n",
        "Implementar el algoritmo PageRank en una clase PagerankDocScorer, que permitirá devolver un ranking de los documentos de manera similar a como hace un Searcher (pero sin recibir una consulta). \n",
        "\n",
        "Se recomienda, al menos inicialmente, llevar a cabo una implementación con la que los valores de PageRank sumen 1, para ayudar a la validación de la misma. Posteriormente, si se desea, se pueden escalar (o no, a criterio del estudiante) los cálculos omitiendo la división por el número total de páginas en el grafo. Será necesario tratar los nodos sumidero tal como se ha explicado en las clases de teoría."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aQE7SBgpk1S"
      },
      "outputs": [],
      "source": [
        "class PagerankDocScorer():\n",
        "    def __init__(self, graphfile, r, n_iter):\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "        f = open(graphfile, \"r\")\n",
        "        lines = f.read().splitlines()\n",
        "        f.close()\n",
        "\n",
        "        self.nodos = set()  # Elementos sin ordenar y sin repetir #\n",
        "        self.salientes: Dict = {}  # Clave: nodo, valor: lista de a quienes apunta #\n",
        "        self.nodos_desde_hacia: Dict = {}  # Clave: nodo, valor: lista de quienes le apuntan #\n",
        "\n",
        "        # Voy viendo donde apunta cada nodo y almacenando los nodos #\n",
        "        for line in lines:\n",
        "            l = line.split('\\t')\n",
        "            self.nodos.add(l[0])\n",
        "            self.nodos.add(l[1])\n",
        "\n",
        "            # Comprobar salidas de cada nodo #\n",
        "\n",
        "            try:\n",
        "                self.nodos_desde_hacia[l[1]].append(l[0])\n",
        "            except:\n",
        "                self.nodos_desde_hacia[l[1]] = [l[0]]\n",
        "\n",
        "            try:\n",
        "                self.salientes[l[0]].append(l[1])\n",
        "            except:\n",
        "                self.salientes[l[0]] = [l[1]]\n",
        "\n",
        "            # Sumideros #\n",
        "\n",
        "            try:\n",
        "                if self.nodos_desde_hacia[l[0]]:\n",
        "                    pass\n",
        "            except:\n",
        "                self.nodos_desde_hacia[l[0]] = []\n",
        "\n",
        "            try:\n",
        "                if self.salientes[l[1]]:\n",
        "                    pass\n",
        "            except:\n",
        "                self.salientes[l[1]] = []\n",
        "\n",
        "\n",
        "        # Guardamos los sumideros #\n",
        "        self.sumideros = []\n",
        "        for nodo in self.salientes.keys():\n",
        "            if len(self.salientes[nodo]) == 0:\n",
        "                self.sumideros.append(nodo)\n",
        "\n",
        "        self.n = len(self.nodos)\n",
        "        self.r = r\n",
        "\n",
        "    def rank(self, cutoff):\n",
        "        p1 = self.r / self.n\n",
        "        p2 = 1 - self.r\n",
        "        res = []\n",
        "        pageRankActual = {}\n",
        "        pageRankAnterior = {}\n",
        "        \n",
        "\n",
        "        # Inicializar valores (se omite la division entre N) #\n",
        "        for n in self.nodos:\n",
        "            pageRankAnterior[n] = 1\n",
        "\n",
        "        # Computar el PageRank iterativamente #\n",
        "        for aux in range(self.n_iter):\n",
        "            for n in self.nodos:\n",
        "                sum = 0\n",
        "                # Se suma cada nodo que va hacia el que iteramos y luego se suman los sumideros #\n",
        "                for origin in self.nodos_desde_hacia[n]:\n",
        "                    sum += pageRankAnterior[origin] / len(self.salientes[origin])\n",
        "                    \n",
        "                for i in self.sumideros:\n",
        "                    sum += pageRankAnterior[i] / self.n\n",
        "\n",
        "                pageRankActual[n] = p1 + p2 * sum\n",
        "\n",
        "            # Guardamos valores PageRank #\n",
        "            for n in pageRankActual.keys():\n",
        "                pageRankAnterior[n] = pageRankActual[n]\n",
        "\n",
        "        # Ordenamos según PageRank #\n",
        "        ordered = dict(sorted(pageRankActual.items(),\n",
        "                              key=lambda item: item[1], reverse=True))\n",
        "        aux = 0\n",
        "        for k, v in ordered.items():\n",
        "            res.append((k, v))\n",
        "            aux += 1\n",
        "            if aux == cutoff:\n",
        "                break\n",
        "        # Devolvemos una lista [(nodo, pageRankScore), (nodo, pageRankScore), etc] #\n",
        "        return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ZT7seCiQiT"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "Para inicializar el PageRank vamos a leer de un archivo y vamos a establecer tres tipos de grupos: los nodos en general, las salidas que tiene un nodo (un nodo a que nodos va) y los nodos que entran en otro nodo (los nodos que llegan a un nodo concreto). Por cada línea del archivo vemos dónde apunta cada uno y contamos las salidas de cada nodo además de las entradas que recibe ese nodo. Finalmente, aquellos nodos que no tienen salidas son considerados sumideros, y por lo tanto los guardamos en una categoría aparte (self.sumideros).\n",
        "\n",
        "Para hacer el rank, inicializamos todos los pageRanks a 1 para que todos los nodos estén igualados al principio. A continuación, por un lado, iteraremos por todos los nodos y sumaremos todos los nodos que van hacia el nodo en el que estamos y, por otro lado, sumaremos los sumideros. Una vez hemos iterado todos los nodos reordenamos en función del PageRank y devolvemos una lista del tamaño del cutoff en la viene el nodo y el PageRank del mismo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zXhPtFzon72"
      },
      "source": [
        "# Programa de prueba **main**\n",
        "\n",
        "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta **collections** en el mismo directorio que este *notebook*. El fichero <u>toys.zip</u> hay que descomprimirlo para indexar las carpetas que contiene. Igualmente, el fichero <u>graphs.zip</u> incluye ficheros (*1k-links.dat*, *toy-graph1.dat*, *toy-graph2.dat*) que se deben descomprimir en la carpeta collections para que el main funcione."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9MDo81ZmK9A"
      },
      "source": [
        "## Función **main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTdDacCRn0u6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import psutil\n",
        "import time\n",
        "\n",
        "def main():\n",
        "    index_root_dir = \"./index/\"\n",
        "    collections_root_dir = \"./collections/\"\n",
        "    test_collection (collections_root_dir + \"toys/toy1/\", index_root_dir + \"toys/toy1/\", \"cc\", [\"aa dd\", \"aa\"], True)\n",
        "    test_collection (collections_root_dir + \"toys/toy2/\", index_root_dir + \"toys/toy2/\", \"aa\", [\"aa cc\", \"bb aa\"], True)\n",
        "    test_collection (collections_root_dir + \"urls.txt\", index_root_dir + \"urls/\", \"wikipedia\", [\"information probability\", \"probability information\", \"higher probability\"], False)\n",
        "    test_collection (collections_root_dir + \"docs1k.zip\", index_root_dir + \"docs1k/\", \"seat\", [\"obama family tree\"], True)\n",
        "    test_collection (collections_root_dir + \"docs10k.zip\", index_root_dir + \"docs10k/\", \"seat\", [\"obama family tree\"], True)\n",
        "    test_pagerank(\"./collections/\", 5)\n",
        "\n",
        "def test_collection(collection_path: str, index_path: str, word: str, queries: list, analyse_performance: bool):\n",
        "    print(\"=================================================================\")\n",
        "    print(\"Testing indices and search on \" + collection_path)\n",
        "\n",
        "    # # We now test building different implementations of an index\n",
        "    test_build(WhooshBuilder(index_path + \"whoosh\"), collection_path)\n",
        "    test_build(WhooshForwardBuilder(index_path + \"whoosh_fwd\"), collection_path)\n",
        "    test_build(WhooshPositionalBuilder(index_path + \"whoosh_pos\"), collection_path)\n",
        "    test_build(RAMIndexBuilder(index_path + \"ram\"), collection_path)\n",
        "    test_build(DiskIndexBuilder(index_path + \"disk\"), collection_path)\n",
        "    test_build(PositionalIndexBuilder(index_path + \"pos\"), collection_path)\n",
        "\n",
        "    # # We now inspect all the implementations\n",
        "    indices = [\n",
        "            WhooshIndex(index_path + \"whoosh\"),\n",
        "            WhooshForwardIndex(index_path + \"whoosh_fwd\"), \n",
        "            WhooshPositionalIndex(index_path + \"whoosh_pos\"), \n",
        "            RAMIndex(index_path + \"ram\"),\n",
        "            DiskIndex(index_path + \"disk\"),\n",
        "            PositionalIndex(index_path + \"pos\"),\n",
        "            ]\n",
        "    for index in indices:\n",
        "        test_read(index, word)\n",
        "\n",
        "    for query in queries:\n",
        "        print(\"------------------------------\")\n",
        "        print(\"Checking search results for %s\" % (query))\n",
        "        # Whoosh searcher can only work with its own indices\n",
        "        test_search(WhooshSearcher(index_path + \"whoosh\"), WhooshIndex(index_path + \"whoosh\"), query, 5)\n",
        "        test_search(WhooshSearcher(index_path + \"whoosh_fwd\"), WhooshForwardIndex(index_path + \"whoosh_fwd\"), query, 5)\n",
        "        test_search(WhooshSearcher(index_path + \"whoosh_pos\"), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
        "        test_search(ProximitySearcher(WhooshPositionalIndex(index_path + \"whoosh_pos\")), WhooshPositionalIndex(index_path + \"whoosh_pos\"), query, 5)\n",
        "        for index in indices:\n",
        "            # our searchers should work with any other index\n",
        "            test_search(SlowVSMSearcher(index), index, query, 5)\n",
        "            test_search(TermBasedVSMSearcher(index), index, query, 5)\n",
        "            test_search(DocBasedVSMSearcher(index), index, query, 5)\n",
        "        test_search(ProximitySearcher(PositionalIndex(index_path + \"pos\")), PositionalIndex(index_path + \"pos\"), query, 5)\n",
        "\n",
        "    # if we keep the list in memory, there may be problems with accessing the same index twice\n",
        "    indices = list()\n",
        "\n",
        "    if analyse_performance:\n",
        "        # let's analyse index performance\n",
        "        test_index_performance(collection_path, index_path)\n",
        "        # let's analyse search performance\n",
        "        for query in queries:\n",
        "            test_search_performance(collection_path, index_path, query, 5)\n",
        "\n",
        "def test_build(builder, collection):\n",
        "    stamp = time.time()\n",
        "    print(\"Building index with\", type(builder))\n",
        "    print(\"Collection:\", collection)\n",
        "    # this function should index the recieved collection and add it to the index\n",
        "    builder.build(collection)\n",
        "    # when we commit, the information in the index becomes persistent\n",
        "    # we can also save any extra information we may need\n",
        "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
        "    builder.commit()\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "def test_read(index, word):\n",
        "    stamp = time.time()\n",
        "    print(\"Reading index with\", type(index))\n",
        "    print(\"Collection size:\", index.ndocs())\n",
        "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
        "    # more tests\n",
        "    doc_id = 0\n",
        "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
        "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
        "    print(\"  Docs containing the word '\" + word + \"':\", index.doc_freq(word))\n",
        "    print(\"    First two documents:\", [(doc, freq) for doc, freq in index.postings(word)][0:2])\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def test_search (engine, index, query, cutoff):\n",
        "    stamp = time.time()\n",
        "    print(\"  \" + engine.__class__.__name__ + \" with index \" + index.__class__.__name__ + \" for query '\" + query + \"'\")\n",
        "    for path, score in engine.search(query, cutoff):\n",
        "        print(score, \"\\t\", path)\n",
        "    print()\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "def disk_space(index_path: str) -> int:\n",
        "    space = 0\n",
        "    for f in os.listdir(index_path):\n",
        "      p = os.path.join(index_path, f)\n",
        "      if os.path.isfile(p):\n",
        "        space += os.path.getsize(p)\n",
        "    return space\n",
        "\n",
        "\n",
        "def test_index_performance (collection_path: str, base_index_path: str):\n",
        "\n",
        "    pid = os.getpid()\n",
        "    meminfo = psutil.Process(pid)\n",
        "    \n",
        "    print(\"----------------------------\")\n",
        "    print(\"Testing index performance on \" + collection_path + \" document collection\")\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"  Build time...\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    b = WhooshBuilder(base_index_path + \"whoosh\")\n",
        "    b.build(collection_path)\n",
        "    b.commit()\n",
        "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    b = WhooshForwardBuilder(base_index_path + \"whoosh_fwd\")\n",
        "    b.build(collection_path)\n",
        "    b.commit()\n",
        "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    b = WhooshPositionalBuilder(base_index_path + \"whoosh_pos\")\n",
        "    b.build(collection_path)\n",
        "    b.commit()\n",
        "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    b = RAMIndexBuilder(base_index_path + \"ram\")\n",
        "    b.build(collection_path)\n",
        "    b.commit()\n",
        "    print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    b = DiskIndexBuilder(base_index_path + \"disk\")\n",
        "    b.build(collection_path)\n",
        "    b.commit()\n",
        "    print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    b = PositionalIndexBuilder(base_index_path + \"positional index\")\n",
        "    b.build(collection_path)\n",
        "    b.commit()\n",
        "    print(\"\\tPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"  Load time...\")\n",
        "    start_time = time.time()\n",
        "    WhooshIndex(base_index_path + \"whoosh\")\n",
        "    print(\"\\tWhooshIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    WhooshForwardIndex(base_index_path + \"whoosh_fwd\")\n",
        "    print(\"\\tWhooshForwardIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    WhooshPositionalIndex(base_index_path + \"whoosh_pos\")\n",
        "    print(\"\\tWhooshPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    RAMIndex(base_index_path + \"ram\")\n",
        "    print(\"\\tRAMIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    DiskIndex(base_index_path + \"disk\")\n",
        "    print(\"\\tDiskIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    PositionalIndex(base_index_path + \"disk\")\n",
        "    print(\"\\tPositionalIndex: %s seconds ---\" % (time.time() - start_time))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    print(\"===============================================================\")\n",
        "    print(\"  Disk space...\")\n",
        "    print(\"\\tWhooshIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh\")))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    print(\"\\tWhooshForwardIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_fwd\")))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    print(\"\\tWhooshPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"whoosh_pos\")))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    print(\"\\tRAMIndex: %s space ---\" % (disk_space(base_index_path + \"ram\")))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    print(\"\\tDiskIndex: %s space ---\" % (disk_space(base_index_path + \"disk\")))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "    print(\"\\tPositionalIndex: %s space ---\" % (disk_space(base_index_path + \"positional index\")))\n",
        "    memoryUse = meminfo.memory_info()[0]/1024/1024\n",
        "    print(f\"\\tMemory usage in MB: {memoryUse}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "def test_search_performance (collection_name: str, base_index_path: str, query: str, cutoff: int):\n",
        "    print(\"----------------------------\")\n",
        "    print(\"Testing search performance on \" + collection_name + \" document collection with query: '\" + query + \"'\")\n",
        "    whoosh_index = WhooshIndex(base_index_path + \"whoosh\")\n",
        "    ram_index = RAMIndex(base_index_path + \"ram\")\n",
        "    disk_index = DiskIndex(base_index_path + \"disk\")\n",
        "    positional_index = PositionalIndex(base_index_path + \"positional index\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    test_search(WhooshSearcher(base_index_path + \"whoosh\"), whoosh_index, query, cutoff)\n",
        "    print(\"--- Whoosh on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "    test_search(SlowVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
        "    print(\"--- SlowVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    # # let's test some combinations of ranking + index implementations\n",
        "    start_time = time.time()\n",
        "    test_search(TermBasedVSMSearcher(whoosh_index), whoosh_index, query, cutoff)\n",
        "    print(\"--- TermVSM on Whoosh %s seconds ---\" % (time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "    test_search(TermBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
        "    print(\"--- TermVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "    test_search(TermBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
        "    print(\"--- TermVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "    test_search(TermBasedVSMSearcher(positional_index), positional_index, query, cutoff)\n",
        "    print(\"--- TermVSM on PositionalIndex %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "    start_time = time.time()\n",
        "    test_search(DocBasedVSMSearcher(ram_index), ram_index, query, cutoff)\n",
        "    print(\"--- DocVSM on RAM %s seconds ---\" % (time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "    test_search(DocBasedVSMSearcher(disk_index), disk_index, query, cutoff)\n",
        "    print(\"--- DocVSM on Disk %s seconds ---\" % (time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "    test_search(DocBasedVSMSearcher(positional_index), positional_index, query, cutoff)\n",
        "    print(\"--- DocVSM on PositionalIndex %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "def test_pagerank(graphs_root_dir, cutoff):\n",
        "    print(\"----------------------------\")\n",
        "    print(\"Testing PageRank\")\n",
        "    # we separate this function because it cannot work with all the collections\n",
        "    start_time = time.time()\n",
        "    for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph1.dat\", 0.5, 50).rank(cutoff):\n",
        "        print(score, \"\\t\", path)\n",
        "    print()\n",
        "    print(\"--- Pagerank with toy_graph_1 %s seconds ---\" % (time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "    for path, score in PagerankDocScorer(graphs_root_dir + \"toy-graph2.dat\", 0.6, 50).rank(cutoff):\n",
        "        print(score, \"\\t\", path)\n",
        "    print()\n",
        "    print(\"--- Pagerank with toy_graph_2 %s seconds ---\" % (time.time() - start_time))\n",
        "    start_time = time.time()\n",
        "    for path, score in PagerankDocScorer(graphs_root_dir + \"1k-links.dat\", 0.2, 50).rank(cutoff):\n",
        "        print(score, \"\\t\", path)\n",
        "    print()\n",
        "    print(\"--- Pagerank with simulated links for doc1k %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5JhJJSFiSl5"
      },
      "source": [
        "### Resumen de coste y rendimiento\n",
        "\n",
        "Para el consumo de RAM no hemos consiserado optimo utillizar un porcentaje debido a que lo hemos ejecutado en un equipo windows por lo que los porcenatajes pueden variar mucho sin contar con el programa ejecutado asique en lugar del porcentaje visto en cada momento hemos apuntado la cantidad de RAM utilizada por el proceso actual, esto lo hemos conseguuido haciendo uso de la libreria psutil, obteniendo el pid del proceso a la hora de hacer los tests y cogiendo los valores de la funcion memory_info() siendo el primer valor el rss del equipo.\n",
        "\n",
        "Hemos optado por dejar los cambios en el main para comparacion de resultados.\n",
        "\n",
        "La medida se ha hecho de forma individual, al ejecutar el ejercicio entero los resultados pueden variar, la entrega se ha hecho ejecutando uno a uno y reiniciando el notebook con cada ejecución\n",
        "\n",
        "El equipo utilizado tiene las siguientes caracteristicas\n",
        "\n",
        "    Total memoria RAM: 16 GB\n",
        "    Procesador: AMD Ryzen 5 5500U with Radeon Graphics 2.10 GHz\n",
        "    Numero de nucleos: 6 Procesadores fisicos (12 Procesadores lógicos)\n",
        "\n",
        " -\n",
        "\n",
        "|  RAMIndex      | Construcción |del | índice | Carga |del  índice |\n",
        "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
        "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
        "| toy1 | 0.004004478454589844 | 80.12890625 MB | 1312 | 0.0009987354278564453 | 80.15234375 MB |\n",
        "| toy2 | 0.004998445510864258 | 80.90234375 MB | 1199 | 0.0010004043579101562 | 80.90625 MB |\n",
        "| 1K | 36.583067417144775 | 233.8515625 MB | 7264763 | 0.9582297801971436 | 340.30078125 MB |\n",
        "| 10K | 260.15782046318054 | 807.95703125 MB | 51154568 | 15.77506160736084 | 1499.16796875 MB |\n",
        "\n",
        " -\n",
        "\n",
        "|   DISKIndex      | Construcción |del | índice | Carga| del  índice |\n",
        "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
        "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
        "| toy1 | 0.05399942398071289 | 80.12890625 MB | 1266 | 0.0019991397857666016 | 80.15234375 MB |\n",
        "| toy2 | 0.004998445510864258 | 80.90234375 MB | 1162 | 0.0019991397857666016 | 80.90625 MB |\n",
        "| 1K | 60.96139931678772 | 239.1015625 MB | 5978805 | 0.04128122329711914 | 340.30078125 MB |\n",
        "| 10K | 335.91592717170715 | 838.42578125 MB | 42905316 | 0.19899725914001465 | 1496.41796875 MB |\n",
        "\n",
        " -\n",
        "\n",
        "|   PositionalIndex   | Construcción |del | índice | Carga |del  índice |\n",
        "|------|--------------------|-----------------|------------------|-----------------|-----------------|\n",
        "|      | Tiempo de indexado | Consumo máx RAM | Espacio en disco | Tiempo de carga | Consumo máx RAM |\n",
        "| toy1 | 0.04098963737487793 | 80.12890625 MB | 13329 | 0.002002239227294922 | 80.15234375 MB |\n",
        "| toy2 | 0.04300403594970703 | 80.90234375 MB | 12642 | 0.0019948482513427734 | 80.90625 MB |\n",
        "| 1K | 38.02010130882263 | 333.89844375 MB | 11907362 | 0.047033071517944336 | 340.30078125 MB |\n",
        "| 10K | 271.85889172554016 | 1489.2421875 MB | 79015017 | 0.201005220413208 | 1497.11328125 MB |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Enunciado P2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
