{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMuz6soJxEmP"
      },
      "source": [
        "### **Búsqueda y Minería de Información 2021-22**\n",
        "### Universidad Autónoma de Madrid, Escuela Politécnica Superior\n",
        "### Grado en Ingeniería Informática, 4º curso\n",
        "# **Implementación de un motor de búsqueda**\n",
        "\n",
        "Fechas:\n",
        "\n",
        "* Comienzo: lunes 7 / martes 8 de febrero\n",
        "* Entrega: lunes 21 / martes 22 de febrero (14:00)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bdu5nO581M"
      },
      "source": [
        "# Introducción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlYtzLZp6KwJ"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "Los objetivos de esta práctica son:\n",
        "\n",
        "* La iniciación a la implementación de un motor de búsqueda.\n",
        "*\tUna primera comprensión de los elementos básicos necesarios para implementar un motor completo.\n",
        "*\tLa iniciación al uso de la librería [Whoosh](https://whoosh.readthedocs.io/en/latest/intro.html) en Python para la creación y utilización de índices, funcionalidades de búsqueda en texto.\n",
        "*\tLa iniciación a la implementación de una función de ránking sencilla.\n",
        "\n",
        "Los documentos que se indexarán en esta práctica, y sobre los que se realizarán consultas de búsqueda serán documentos HTML, que deberán ser tratados para extraer y procesar el texto contenido en ellos. \n",
        "\n",
        "La práctica plantea como punto de partida una pequeña API general sencilla, que pueda implementarse de diferentes maneras, como así se hará en esta práctica y las siguientes. A modo de toma de contacto y arranque de la asignatura, en esta primera práctica se completará una implementación de la API utilizando Whoosh, con lo que resultará bastante trivial la solución (en cuanto a la cantidad de código a escribir). En la siguiente práctica el estudiante desarrollará sus propias implementaciones, sustituyendo el uso de Whoosh que vamos a hacer en esta primera práctica.\n",
        "\n",
        "En términos de operaciones propias de un motor de búsqueda, en esta práctica el estudiante se encargará fundamentalmente de:\n",
        "\n",
        "a) En el proceso de indexación: recorrer los documentos de texto de una colección dada, eliminar del contenido posibles marcas tales como html, y enviar el texto a indexar por parte de Whoosh. \n",
        "\n",
        "b) En el proceso de responder consultas: implementar una primera versión sencilla de una o dos funciones de ránking en el modelo vectorial, junto con alguna pequeña estructura auxiliar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AjDofIw6Ns6"
      },
      "source": [
        "## Material proporcionado\n",
        "\n",
        "Se proporcionan (bien en el curso de Moodle o dentro de este documento):\n",
        "\n",
        "*\tVarias clases e interfaces Python a lo largo de este *notebook*, desde las que el estudiante partirá para completar código e integrará las suyas propias. \n",
        "En particular, la función **main** implementa un programa que deberá funcionar con el código a implementar por el estudiante. Además, se proporciona una celda con código que muestra las funciones más útiles de la API de Whoosh.\n",
        "*\tUna pequeña colección <ins>docs1k.zip</ins> con aproximadamente 1.000 documentos HTML, y un pequeño fichero <ins>urls.txt</ins>. Ambas representan colecciones de prueba para depurar las implementaciones y comprobar su corrección.\n",
        "*\tUn documento de texto <ins>output.txt</ins> con la salida estándar que deberá producir la ejecución de la función main."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9udSskn_eY7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    This is an abstract class for the search engines\n",
        "\"\"\"\n",
        "class Searcher(ABC):\n",
        "    def __init__(self, index):\n",
        "        self.index = index\n",
        "    @abstractmethod\n",
        "    def search(self, query, cutoff):\n",
        "        \"\"\" Returns a list of documents built as a pair of path and score \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Whoosh API\n",
        "import whoosh\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "from whoosh.formats import Format\n",
        "from whoosh.qparser import QueryParser\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import os, os.path\n",
        "import shutil\n",
        "\n",
        "Document = Schema(\n",
        "        path=ID(stored=True),\n",
        "        content=TEXT(vector=Format))\n",
        "\n",
        "def whooshexample_buildindex(dir, urls):\n",
        "    if os.path.exists(dir): shutil.rmtree(dir)\n",
        "    os.makedirs(dir)\n",
        "    writer = whoosh.index.create_in(dir, Document).writer()\n",
        "    for url in urls:\n",
        "        writer.add_document(path=url, content=BeautifulSoup(urlopen(url).read(), \"lxml\").text)\n",
        "    writer.commit()\n",
        "\n",
        "def whooshexample_search(dir, query):\n",
        "    index = whoosh.index.open_dir(dir)\n",
        "    searcher = index.searcher()\n",
        "    qparser = QueryParser(\"content\", schema=index.schema)\n",
        "    print(\"Search results for '\", query, \"'\")\n",
        "    for docid, score in searcher.search(qparser.parse(query)).items():\n",
        "        print(score, \"\\t\", index.reader().stored_fields(docid)['path'])\n",
        "    print()\n",
        "\n",
        "def whooshexample_examine(dir, term, docid, n):\n",
        "    reader = whoosh.index.open_dir(dir).reader()\n",
        "    print(\"Total nº of documents in the collection:\", reader.doc_count())\n",
        "    print(\"Total frequency of '\", term, \"':\", reader.frequency(\"content\", term))\n",
        "    print(\"Nº documents containing '\", term, \"':\", reader.doc_frequency(\"content\", term))\n",
        "    for p in reader.postings(\"content\", term).items_as(\"frequency\") if reader.doc_frequency(\"content\", term) > 0 else []:\n",
        "        print(\"\\tFrequency of '\", term, \"' in document\", p[0], \":\", p[1])\n",
        "    raw_vec = reader.vector(docid, \"content\")\n",
        "    raw_vec.skip_to(term)\n",
        "    if raw_vec.id() == term:\n",
        "        print(\"Frequency of '\", raw_vec.id(), \"' in document\", docid, reader.stored_fields(docid)['path'], \":\", raw_vec.value_as(\"frequency\"))\n",
        "    else:\n",
        "        print(\"Term '\", term, \"' not found in document\", docid)\n",
        "    print(\"Top\", n, \"most frequent terms in document\", docid, reader.stored_fields(docid)['path']) \n",
        "    vec = reader.vector(docid, \"content\").items_as(\"frequency\")\n",
        "    for p in sorted(vec, key=lambda x: x[1], reverse=True)[0:n]:\n",
        "        print(\"\\t\", p)\n",
        "    print()\n",
        "\n",
        "urls = [\"https://en.wikipedia.org/wiki/Simpson's_paradox\", \n",
        "        \"https://en.wikipedia.org/wiki/Bias\",\n",
        "        \"https://en.wikipedia.org/wiki/Entropy\"]\n",
        "\n",
        "dir = \"index/whoosh/example/urls\"\n",
        "\n",
        "whooshexample_buildindex(dir, urls)\n",
        "whooshexample_search(dir, \"probability\")\n",
        "whooshexample_examine(dir, \"probability\", 0, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ciKzD4D6Xn6"
      },
      "source": [
        "## Calificación\n",
        "\n",
        "Esta práctica se calificará con una puntuación de 0 a 10 atendiendo a las puntuaciones individuales de ejercicios y apartados dadas en el enunciado.  \n",
        "\n",
        "El peso de la nota de esta práctica en la calificación final de prácticas es del **20%**.\n",
        "\n",
        "La calificación se basará en a) el **número** de ejercicios realizados y b) la **calidad** de los mismos. \n",
        "La puntuación que se indica en cada apartado es orientativa, en principio se aplicará tal cual se refleja pero podrá matizarse por criterios de buen sentido si se da el caso.\n",
        "\n",
        "Para dar por válida la realización de un ejercicio, el código deberá funcionar (a la primera) integrado con las clases que se facilitan, **sin ninguna modificación**. El profesor comprobará este aspecto añadiendo los módulos entregados por el estudiante a los módulos facilitados en la práctica, ejecutando la función main así como otros main de prueba adicionales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnln3zQV6anE"
      },
      "source": [
        "## Entrega\n",
        "\n",
        "La entrega consistirá en un único fichero tipo *notebook* donde se incluirán todas las **implementaciones** solicitadas en cada ejercicio, así como una explicación de cada uno a modo de **memoria**.\n",
        "\n",
        "En concreto, se debe documentar:\n",
        "\n",
        "- Qué version(es) del modelo vectorial se ha(n) implementado en el ejercicio 2.\n",
        "- Cómo se ha conseguido colocar un documento en la primera posición de ránking, para cada buscador implementado en el ejercicio 2.\n",
        "- El trabajo realizado en el ejercicio 3. \n",
        "- Y cualquier otro aspecto que el estudiante considere oportuno destacar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GtUWMA76bP0"
      },
      "source": [
        "## Indicaciones\n",
        "\n",
        "Se podrán definir clases adicionales a las que se indican en el enunciado, por ejemplo, para reutilizar código. Y el estudiante podrá utilizar o no el software que se le proporciona, con la siguiente limitación: \n",
        "\n",
        "*\tNo deberá editarse el software proporcionado más allá de donde se indica explícitamente.\n",
        "*\tEl programa **main deberá ejecutar** correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gICjfJ-B6g0Y"
      },
      "source": [
        "# Ejercicio 1: Implementación basada en Whoosh\n",
        "\n",
        "Implementar las clases y módulos necesarios para que el programa main funcione. Se deja al estudiante deducir alguna de las relaciones jerárquicas entre las clases Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m18qmDwAzANn"
      },
      "source": [
        "## Ejercicio 1.1: Indexación (3pt)\n",
        "\n",
        "Definir las siguientes clases:\n",
        "\n",
        "* Index.\n",
        "* WhooshIndex, como subclase de Index.\n",
        "*\tWhooshBuilder, como subclase de Builder.\n",
        "\n",
        "Se sugiere utilizar dos “fields” en el esquema de documentos de Whoosh: la ruta del documento (dirección Web o ruta en disco), y el contenido del documento.\n",
        "\n",
        "La entrada para construir el índice (método Builder.build()) podrá ser a) un fichero de texto con direcciones Web (una por línea); b) una carpeta del disco (se indexarán todos los ficheros de la carpeta, sin entrar en subcarpetas); o c) un archivo zip que contiene archivos comprimidos a indexar. Supondremos que el contenido a indexar es siempre HTML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiNJ9ru19cN0"
      },
      "outputs": [],
      "source": [
        "class Index:\n",
        "    def __init__(self, path):\n",
        "        pass\n",
        "\n",
        "    def doc_vector(self, doc_id):\n",
        "        pass\n",
        "\n",
        "    def ndocs(self):\n",
        "        pass\n",
        "\n",
        "    def all_terms(self):\n",
        "        pass\n",
        "\n",
        "    def all_terms_with_freq(self):\n",
        "        pass\n",
        "\n",
        "    def total_freq(self, term):\n",
        "        pass\n",
        "\n",
        "    def doc_path(self, id):\n",
        "        pass\n",
        "\n",
        "    def term_freq(self, term, id):\n",
        "        pass\n",
        "\n",
        "    def doc_freq(self, term):\n",
        "        pass\n",
        "\n",
        "    def postings(self, term):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eu23hSs6_wvX"
      },
      "outputs": [],
      "source": [
        "import whoosh\n",
        "from whoosh.fields import Schema, TEXT, ID\n",
        "from whoosh.formats import Format\n",
        "from whoosh.qparser import QueryParser\n",
        "import os, os.path\n",
        "import shutil\n",
        "from bs4 import BeautifulSoup\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "#from index import Builder, Index\n",
        "#from searcher import Searcher\n",
        "\n",
        "# A schema in Whoosh is the set of possible fields in a document in\n",
        "# the search space. We just define a simple 'Document' schema\n",
        "Document = Schema(\n",
        "        path=ID(stored=True),\n",
        "        content=TEXT(vector=Format)\n",
        "        )\n",
        "\n",
        "\n",
        "class Builder:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def build(collection):\n",
        "        pass\n",
        "\n",
        "    def commit(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class WhooshBuilder(Builder):\n",
        "    def __init__(self, dir):\n",
        "        if os.path.exists(dir): shutil.rmtree(dir)\n",
        "        os.makedirs(dir)\n",
        "        self.writer = whoosh.index.create_in(dir, Document).writer()\n",
        "\n",
        "    def build(self, collection):\n",
        "        if os.path.isfile(collection):\n",
        "            # Check if it is a normal file or zip file or a folder#\n",
        "            if collection.endswith(\".txt\"):\n",
        "                with open(collection) as file:\n",
        "                    url_list = file.readlines()\n",
        "                    for url in url_list:\n",
        "                        self.writer.add_document(path=url, content=BeautifulSoup(urlopen(url).read(), \"lxml\").text)\n",
        "                return\n",
        "            elif collection.endswith(\".zip\"):\n",
        "                url_zip_list = ZipFile(collection, 'r')\n",
        "                for urls in url_zip_list.namelist():\n",
        "                        self.writer.add_document(path=urls, content=BeautifulSoup(url_zip_list.read(urls), \"lxml\").text)\n",
        "                return\n",
        "\n",
        "        else:\n",
        "            # Folder\n",
        "            dir_urls = os.listdir(collection)\n",
        "            for document in sorted(dir_urls):\n",
        "                path = os.path.join(collection, document)\n",
        "                with open(path, 'r') as url:\n",
        "                    self.writer.add_document(path=path, content=url.read())\n",
        "\n",
        "    def commit(self):\n",
        "        self.writer.commit()\n",
        "\n",
        "\n",
        "class WhooshIndex(Index):\n",
        "    def __init__(self, path):\n",
        "        self.index_path = path\n",
        "        self.index = whoosh.index.open_dir(path)\n",
        "        self.reader = whoosh.index.open_dir(path).reader()\n",
        "\n",
        "    def doc_vector(self, doc_id):\n",
        "        vector: List = []\n",
        "        raw_vec = self.reader.vector_as(\"frequency\", doc_id, \"content\")\n",
        "\n",
        "        for t in raw_vec:\n",
        "            term = t[0]\n",
        "            freq = t[1]\n",
        "            vector.append((term, freq))\n",
        "\n",
        "        return vector\n",
        "\n",
        "    def ndocs(self):\n",
        "        return self.reader.doc_count()\n",
        "\n",
        "    def all_terms(self):\n",
        "        terms = self.reader.all_terms()\n",
        "        list_terms: List = []\n",
        "        for fieldname, text in terms:\n",
        "            if fieldname == \"content\":\n",
        "                list_terms.append(text.decode(\"utf-8\"))\n",
        "\n",
        "        return list_terms\n",
        "\n",
        "    def all_terms_with_freq(self):\n",
        "        list_terms = self.all_terms()\n",
        "        terms_freq: List = []\n",
        "        for term in list_terms:\n",
        "            terms_freq.append((term, self.total_freq(term)))\n",
        "\n",
        "        return terms_freq\n",
        "\n",
        "    def total_freq(self, term):\n",
        "        return self.reader.frequency(\"content\", term)\n",
        "\n",
        "    def doc_path(self, doc_id):\n",
        "        return self.reader.stored_fields(doc_id)['path']\n",
        "\n",
        "    def term_freq(self, term, doc_id):\n",
        "        counter = self.reader.vector(doc_id, \"content\")\n",
        "        counter.skip_to(term)\n",
        "        return counter.value_as(\"frequency\") if counter.id() == term else 0\n",
        "\n",
        "    def doc_freq(self, term):\n",
        "        return self.reader.doc_frequency(\"content\", term)\n",
        "\n",
        "    def postings(self, word):\n",
        "        res: List = []\n",
        "        try:\n",
        "            pr = self.reader.postings(\"content\", word)\n",
        "        except Exception:\n",
        "            return res\n",
        "\n",
        "        cont = 0\n",
        "\n",
        "        for cont in range(self.ndocs()):\n",
        "            try:\n",
        "                pr.skip_to(cont)\n",
        "                if pr.id() == cont:\n",
        "                    freq = self.term_freq(word, cont)\n",
        "                    res.append((cont, freq))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGjT-DbW-yHR"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "Al ser WhooshIndex y WhooshBuilder subclases de Index y Builder, crearemos los métodos en las dos primeras clases y se instanciarán desde Index y Builder (de esta froma se consigue la encapsulacion). Se utilizan las funciones proporcionadas por la api Whoosh en relación a contar número de documentos o sacar la frecuencia de un término."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEOpKvZi9cos"
      },
      "source": [
        "## Ejercicio 1.2: Búsqueda (1.5pt)\n",
        "\n",
        "Implementar la clase WhooshSearcher como subclase de Searcher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IGtH0CpF9owH"
      },
      "outputs": [],
      "source": [
        "class WhooshSearcher(Searcher):\n",
        "    def __init__(self, index):\n",
        "        self.index = whoosh.index.open_dir(index)\n",
        "        self.searcher = self.index.searcher()\n",
        "        self.qparser = QueryParser(\"content\", schema=Document)\n",
        "\n",
        "    def search(self, query, cutoff):\n",
        "        \"\"\" Returns a list of documents built as a pair of path and score \"\"\"\n",
        "        list_docs: List = []\n",
        "        counter = 0\n",
        "        for docid, score in self.searcher.search(self.qparser.parse(query)).items():\n",
        "            if counter > cutoff:\n",
        "                break\n",
        "            doc_path = self.index.reader().stored_fields(docid)['path']\n",
        "            list_docs.append((doc_path, score))\n",
        "            counter = counter + 1\n",
        "\n",
        "        return list_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNDHrfjN-zTV"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "La implementación del primer ejercicio se ha hecho en base al fragmento proporcionado como ejemplo. Con la anterior implementación del index y el builder tuvimos mayor soltura a la hora de implementar el searcher, utilizando como base searcher de whoosh y realizando el formateo necesario para obtener la salida deseada.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH9u7bWi9pGc"
      },
      "source": [
        "# Ejercicio 2: Modelo vectorial\n",
        "\n",
        "Implementar dos modelos de ránking propios, basados en el modelo vectorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SphKCsSN9udY"
      },
      "source": [
        "## Ejercicio 2.1: Producto escalar (2pt)\n",
        "\n",
        "Implementar un modelo vectorial propio que utilice el producto escalar (sin dividir por las normas de los vectores) como función de ránking, por medio de la clase VSMDotProductSearcher, como subclase de Searcher.\n",
        "\n",
        "Este modelo hará uso de la clase Index y se podrá probar con la implementación WhooshIndex (puedes ver un ejemplo de esto en la función main).\n",
        "\n",
        "Además, la clase VSMDotProductSearcher será intercambiable con WhooshSearcher, como se puede ver en main, donde la función test_search utiliza una implementación u otra sin distinción.\n",
        "\n",
        "Para simplificar, aplicar a las consultas simplemente una separación de palabras por espacios en blanco y normalización a minúsculas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JQ2xbth-A-G"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "from typing import Dict, List\n",
        "\n",
        "def tf(freq):\n",
        "    return 1 + math.log2(freq) if freq > 0 else 0\n",
        "\n",
        "\n",
        "def idf(df, n):\n",
        "    return math.log2((n + 1) / (df + 0.5))\n",
        "\n",
        "\n",
        "def calc_module(index):\n",
        "    array_modules: List = []\n",
        "    idf_values: Dict = {}\n",
        "\n",
        "    for t in index.all_terms():\n",
        "        idf_values[t] = idf(index.doc_freq(t), index.ndocs())\n",
        "\n",
        "    for doc_id in range(index.ndocs()):\n",
        "        doc_tf_sum = 0\n",
        "        for doc_inf_term, doc_inf_freq in index.doc_vector(doc_id):\n",
        "            if doc_inf_term in idf_values:\n",
        "                doc_tf_sum += (tf(doc_inf_freq) * idf_values[doc_inf_term]) ** 2\n",
        "\n",
        "        array_modules.append(math.sqrt(doc_tf_sum))\n",
        "\n",
        "    return array_modules\n",
        "\n",
        "\n",
        "class VSMDotProductSearcher(Searcher):\n",
        "    def __init__(self, index):\n",
        "        self.index = index\n",
        "\n",
        "    def search(self, query, limit):\n",
        "        # Parseamos y eliminamos mayusculas, dividimos por espacios\n",
        "        qterms = query.lower().split(' ')\n",
        "        results: List = []\n",
        "\n",
        "        for doc_id in range(self.index.ndocs()):\n",
        "            tf_idf = 0\n",
        "            for q_word in qterms:\n",
        "                tf_idf += self.score(q_word, doc_id)\n",
        "            if tf_idf > 0:\n",
        "                results.append([self.index.doc_path(doc_id), tf_idf])\n",
        "        results.sort(key=lambda results: results[1], reverse=True)\n",
        "        return results[0:limit]\n",
        "\n",
        "    def score(self, term, doc_id):\n",
        "        return tf(self.index.term_freq(term, doc_id)) * idf(self.index.doc_freq(term), self.index.ndocs())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgumTPoT-1WD"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "La implementación del modelo vectorial se ha realizado siguiendo las formulas vistas en clase, implementando los datos tf e idf solo para los terminos de la consulta y realizando el sumatorio tendriamos el numerador de la formula del coseno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o5TvvE5-BVK"
      },
      "source": [
        "### Ejercicio\n",
        "\n",
        "Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n",
        "\n",
        "Para esto, debemos hacer que el documento sea el más relacionado con la consulta.\n",
        "\n",
        "Para esto se ha creado un fichero first_rank.html el cual contiene las palabras \"obama family tree\" repetidas varias veces para conseguir que tf aumente considerablemente.\n",
        "\n",
        "\n",
        "------------------------------\n",
        "Checking search results\n",
        "\n",
        "  WhooshSearcher for query 'obama family tree'\n",
        "\n",
        "16.862884703494828       first_rank.html\n",
        "\n",
        "16.478676012011036       clueweb09-en0010-79-2218.html\n",
        "\n",
        "15.882079096998659       clueweb09-en0010-57-32937.html\n",
        "\n",
        "15.80345918863734        clueweb09-en0001-02-21241.html\n",
        "\n",
        "15.60644775939374        clueweb09-en0008-45-29117.html\n",
        "\n",
        "15.54975407819375        clueweb09-enwp01-59-16163.html\n",
        "\n",
        "\n",
        "Done ( 0.08564496040344238 seconds )\n",
        "\n",
        "\n",
        "  VSMDotProductSearcher for query 'obama family tree'\n",
        "\n",
        "81.34144078844506        first_rank.html\n",
        "\n",
        "49.55667349970334        clueweb09-enwp01-59-16163.html\n",
        "\n",
        "49.55667349970334        clueweb09-enwp02-06-15081.html\n",
        "\n",
        "49.54379747563924        clueweb09-enwp03-00-6901.html\n",
        "\n",
        "49.54379747563924        clueweb09-enwp03-07-2998.html\n",
        "\n",
        "\n",
        "Done ( 0.7998659610748291 seconds )\n",
        "\n",
        "\n",
        "  VSMCosineSearcher for query 'obama family tree'\n",
        "\n",
        "1.5909785228334314       first_rank.html\n",
        "\n",
        "0.28384635267498437      clueweb09-en0010-79-2218.html\n",
        "\n",
        "0.22583504905188845      clueweb09-en0009-30-2768.html\n",
        "\n",
        "0.22454247611890624      clueweb09-en0001-02-21241.html\n",
        "\n",
        "0.22339661397902683      clueweb09-en0009-30-2441.html\n",
        "\n",
        "\n",
        "Done ( 0.8604700565338135 seconds )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPPNbWNe-HcR"
      },
      "source": [
        "## Ejercicio 2.2: Coseno (1.5pt)\n",
        "\n",
        "Refinar la implementación del modelo para que calcule el coseno, definiendo para ello una clase VSMCosineSearcher. Para ello se necesitará extender WhooshBuilder con el cálculo de los módulos de los vectores, que deberán almacenarse en un fichero, en la carpeta de índice junto a los ficheros que genera Whoosh. \n",
        "\n",
        "Pensad en qué parte del diseño interesa hacer esto, en concreto, qué clase y en qué momento tendría que calcular, devolver y/o almacenar estos módulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n30WQX_-RMR"
      },
      "outputs": [],
      "source": [
        "class VSMCosineSearcher(VSMDotProductSearcher):\n",
        "    def __init__(self, index):\n",
        "        self.index = index\n",
        "        self.module = calc_module(index)\n",
        "\n",
        "    def score(self, term, doc_id):\n",
        "        return (tf(self.index.term_freq(term, doc_id)) * idf(self.index.doc_freq(term), self.index.ndocs())) / self.module[doc_id]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJqVKt6o-2oB"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "Haciendo uso de la herencia utilizamos la implementación de search realizada en el anterior apartado tambien para este ya que es equivalente, teniendo que cambiar solo el score.\n",
        "Para realizar esto, dividimos el numerador entre el módulo. El módulo debe hacerse teniendo en cuenta todos los términos del vocabulario, por tanto calcularemos idf de cada uno, y luego para cada documento tendremos que calcular tf solamente si el término está contenido en el documento, en caso contrario no sumaremos nada y así también se ahorra en tiempo de ejecución.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh83cdIu-Re1"
      },
      "source": [
        "### Ejercicio\n",
        "\n",
        "Añadir a mano un documento a la colección docs1k.zip de manera que aparezca el primero para la consulta “obama family tree” para este buscador. Documentar cómo se ha conseguido y por qué resulta así.\n",
        "\n",
        "Al igual que en el caso anterior se ha utilizado el mismo fichero first rank consiguiendo tenerlo en el primer ranking\n",
        "\n",
        "\n",
        "------------------------------\n",
        "Checking search results\n",
        "\n",
        "  WhooshSearcher for query 'obama family tree'\n",
        "\n",
        "16.862884703494828       first_rank.html\n",
        "\n",
        "16.478676012011036       clueweb09-en0010-79-2218.html\n",
        "\n",
        "15.882079096998659       clueweb09-en0010-57-32937.html\n",
        "\n",
        "15.80345918863734        clueweb09-en0001-02-21241.html\n",
        "\n",
        "15.60644775939374        clueweb09-en0008-45-29117.html\n",
        "\n",
        "15.54975407819375        clueweb09-enwp01-59-16163.html\n",
        "\n",
        "\n",
        "\n",
        "Done ( 0.08564496040344238 seconds )\n",
        "\n",
        "\n",
        "  VSMDotProductSearcher for query 'obama family tree'\n",
        "  \n",
        "81.34144078844506        first_rank.html\n",
        "\n",
        "49.55667349970334        clueweb09-enwp01-59-16163.html\n",
        "\n",
        "49.55667349970334        clueweb09-enwp02-06-15081.html\n",
        "\n",
        "49.54379747563924        clueweb09-enwp03-00-6901.html\n",
        "\n",
        "49.54379747563924        clueweb09-enwp03-07-2998.html\n",
        "\n",
        "\n",
        "\n",
        "Done ( 0.7998659610748291 seconds )\n",
        "\n",
        "\n",
        "  VSMCosineSearcher for query 'obama family tree'\n",
        "\n",
        "1.5909785228334314       first_rank.html\n",
        "\n",
        "0.28384635267498437      clueweb09-en0010-79-2218.html\n",
        "\n",
        "0.22583504905188845      clueweb09-en0009-30-2768.html\n",
        "\n",
        "0.22454247611890624      clueweb09-en0001-02-21241.html\n",
        "\n",
        "0.22339661397902683      clueweb09-en0009-30-2441.html\n",
        "\n",
        "\n",
        "\n",
        "Done ( 0.8604700565338135 seconds )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wf1RFu8-V8P"
      },
      "source": [
        "# Ejercicio 3: Estadísticas de frecuencias (2pt)\n",
        "\n",
        "Utilizando las funcionalidades de la clase Index, implementar una función term_stats que calcule a) las frecuencias totales en la colección de los términos, ordenadas de mayor a menor, y b) el número de documentos que contiene cada término, igualmente de mayor a menor. Visualizar las estadísticas obtenidas en dos gráficas en escala log-log (por cada colección –seis gráficas en total), que se mostrarán en el cuaderno entregado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2EgV-4w-erd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "def term_stats(index):\n",
        "    output_dir = \"statics_pdfs/\"\n",
        "    collection = os.path.basename(index.index_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    terms_freqs = index.all_terms_with_freq()\n",
        "    terms_freqs.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "    terms: List = []\n",
        "    total_freqs: List = []\n",
        "    for term in terms_freqs:\n",
        "        terms.append(term[0])\n",
        "        total_freqs.append(term[1])\n",
        "\n",
        "    figure = pyplot.figure()\n",
        "    pyplot.plot(terms, total_freqs)\n",
        "    pyplot.xscale(\"log\")\n",
        "    pyplot.yscale(\"log\")\n",
        "    pyplot.xlabel(\"Terms frequency\")\n",
        "    pyplot.ylabel(\"Frequency of term in collection\")\n",
        "    pyplot.title(\"Term frequency in -> {}\".format(collection))\n",
        "\n",
        "    figure.savefig(os.path.join(output_dir, '{}_term_Freq.pdf'.format(collection)))\n",
        "\n",
        "    all_terms = index.all_terms()\n",
        "    doc_freqs: List = []\n",
        "\n",
        "    for term in all_terms:\n",
        "        doc_freqs.append((term, index.doc_freq(term)))\n",
        "    doc_freqs.sort(key=lambda tup: tup[1], reverse=True)\n",
        "\n",
        "    terms: List = []\n",
        "    doc_freqs_list: List = []\n",
        "    for term in doc_freqs:\n",
        "        terms.append(term[0])\n",
        "        doc_freqs_list.append(term[1])\n",
        "\n",
        "    figure = pyplot.figure()\n",
        "    pyplot.plot(terms, doc_freqs_list)\n",
        "    pyplot.xscale(\"log\")\n",
        "    pyplot.yscale(\"log\")\n",
        "    pyplot.xlabel(\"Doc frequency\")\n",
        "    pyplot.ylabel(\"Docs with term\")\n",
        "    pyplot.title(\"Doc frequency in -> {}\".format(collection))\n",
        "\n",
        "    figure.savefig(os.path.join(output_dir, '{}_doc_Freq.pdf'.format(collection)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlXLFNH3-4MH"
      },
      "source": [
        "### Explicación/documentación\n",
        "\n",
        "Para la creación de las graficas se ha utilizado matplot ya que es una libreria familiar de otras prácticas, la implementación ha sido sencilla ya que se tenian las funciones encargadas de obtener los datos de los documentos, solo ha sido necesario formatear el plot conforme a los datos obtenidos.\n",
        "Los datos son guardados en los ficheros PDF comenzando con el nombre del fichero seguido de \"_doc_freq\" o \"_term_freq\" en funcion de los datos mostrados. La carpeta en la que se guardan los ficheros se denomina \"statics_pdfs\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfgNDMM6-e7k"
      },
      "source": [
        "# Programa de prueba **main**\n",
        "\n",
        "Descarga los ficheros del curso de Moodle y coloca sus contenidos en una carpeta *collections* en el mismo directorio que este *notebook*. El fichero *toy.zip* hay que descomprimirlo para indexar la carpeta que contiene."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op6MnWWE_FMj"
      },
      "source": [
        "## Función **main**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXS8648MzPO3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "#from statics import terms_stats\n",
        "#from searcher import VSMDotProductSearcher, VSMCosineSearcher\n",
        "#from whooshmethods import WhooshBuilder, WhooshIndex, WhooshSearcher\n",
        "\n",
        "\n",
        "def main():\n",
        "    index_root_dir = \"./index/\"\n",
        "    collections_root_dir = \"./collections/\"\n",
        "    test_collection (collections_root_dir + \"toy/\", index_root_dir + \"toy\", \"cc\", \"aa dd\")\n",
        "    test_collection (collections_root_dir + \"urls.txt\", index_root_dir + \"urls\", \"wikipedia\", \"information probability\")\n",
        "    test_collection (collections_root_dir + \"docs1k.zip\", index_root_dir + \"docs\", \"seat\", \"obama family tree\")\n",
        "\n",
        "def clear (index_path: str):\n",
        "    if os.path.exists(index_path): shutil.rmtree(index_path)\n",
        "    else: print(\"Creating \" + index_path)\n",
        "    os.makedirs(index_path)\n",
        "\n",
        "def test_collection(collection_path: str, index_path: str, word: str, query: str):\n",
        "    start_time = time.time()\n",
        "    print(\"=================================================================\")\n",
        "    print(\"Testing indices and search on \" + collection_path)\n",
        "\n",
        "    # Let's create the folder if it did not exist\n",
        "    # and delete the index if it did\n",
        "    clear(index_path)\n",
        "\n",
        "    # We now test building an index\n",
        "    test_build(WhooshBuilder(index_path), collection_path)\n",
        "\n",
        "    # We now inspect the index\n",
        "    index = WhooshIndex(index_path)\n",
        "    test_read(index, word)\n",
        "    term_stats(index) # comando para sacar las graficas\n",
        "\n",
        "    print(\"------------------------------\")\n",
        "    print(\"Checking search results\")\n",
        "    test_search(WhooshSearcher(index_path), query, 5)\n",
        "    test_search(VSMDotProductSearcher(WhooshIndex(index_path)), query, 5)\n",
        "    test_search(VSMCosineSearcher(WhooshIndex(index_path)), query, 5)\n",
        "\n",
        "def test_build(builder, collection):\n",
        "    stamp = time.time()\n",
        "    print(\"Building index with\", type(builder))\n",
        "    print(\"Collection:\", collection)\n",
        "    # this function should index the recieved collection and add it to the index\n",
        "    builder.build(collection)\n",
        "    # when we commit, the information in the index becomes persistent\n",
        "    # we can also save any extra information we may need\n",
        "    # (and that cannot be computed until the entire collection is scanned/indexed)\n",
        "    builder.commit()\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "def test_read(index, word):\n",
        "    stamp = time.time()\n",
        "    print(\"Reading index with\", type(index))\n",
        "    print(\"Collection size:\", index.ndocs())\n",
        "    print(\"Vocabulary size:\", len(index.all_terms()))\n",
        "    terms = index.all_terms_with_freq()\n",
        "    terms.sort(key=lambda tup: tup[1], reverse=True)\n",
        "    print(\"  Top 5 most frequent terms:\")\n",
        "    for term in terms[0:5]:\n",
        "        print(\"\\t\" + term[0] + \"\\t\" + str(term[1]) + \"=\" + str(index.total_freq(term)))\n",
        "    print()\n",
        "    # more tests\n",
        "    doc_id = 0\n",
        "    print()\n",
        "    print(\"  Frequency of word \\\"\" + word + \"\\\" in document \" + str(doc_id) + \" - \" + index.doc_path(doc_id) + \": \" + str(index.term_freq(word, doc_id)))\n",
        "    print(\"  Total frequency of word \\\"\" + word + \"\\\" in the collection: \" + str(index.total_freq(word)) + \" occurrences over \" + str(index.doc_freq(word)) + \" documents\")\n",
        "    print(\"  Docs containing the word'\" + word + \"':\", index.doc_freq(word))\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "\n",
        "def test_search (engine, query, cutoff):\n",
        "    stamp = time.time()\n",
        "    print(\"  \" + engine.__class__.__name__ + \" for query '\" + query + \"'\")\n",
        "    for path, score in engine.search(query, cutoff):\n",
        "        print(score, \"\\t\", path)\n",
        "    print()\n",
        "    print(\"Done (\", time.time() - stamp, \"seconds )\")\n",
        "    print()\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTtUTF7j_QdF"
      },
      "source": [
        "### Salida obtenida por el estudiante\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "=================================================================\n",
        "Testing indices and search on ./collections/toy/\n",
        "Building index with <class 'whooshmethods.WhooshBuilder'>\n",
        "Collection: ./collections/toy/\n",
        "Done ( 0.16936922073364258 seconds )\n",
        "\n",
        "Reading index with <class 'whooshmethods.WhooshIndex'>\n",
        "Collection size: 4\n",
        "Vocabulary size: 39\n",
        "  Top 5 most frequent terms:\n",
        "        aa      9.0=9.0\n",
        "        bb      5.0=5.0\n",
        "        sleep   5.0=5.0\n",
        "        cc      3.0=3.0\n",
        "        die     2.0=2.0\n",
        "\n",
        "\n",
        "  Frequency of word \"cc\" in document 0 - ./collections/toy/d1.txt: 2\n",
        "  Total frequency of word \"cc\" in the collection: 3.0 occurrences over 2 documents\n",
        "  Docs containing the word'cc': 2\n",
        "Done ( 0.0026397705078125 seconds )\n",
        "\n",
        "------------------------------\n",
        "Checking search results\n",
        "  WhooshSearcher for query 'aa dd'\n",
        "\n",
        "Done ( 0.021887540817260742 seconds )\n",
        "\n",
        "  VSMDotProductSearcher for query 'aa dd'\n",
        "4.0      ./collections/toy/d1.txt\n",
        "1.7369655941662063       ./collections/toy/d2.txt\n",
        "1.0      ./collections/toy/d3.txt\n",
        "\n",
        "Done ( 0.0007722377777099609 seconds )\n",
        "\n",
        "  VSMCosineSearcher for query 'aa dd'\n",
        "1.0      ./collections/toy/d2.txt\n",
        "0.7427813527082074       ./collections/toy/d1.txt\n",
        "0.5773502691896258       ./collections/toy/d3.txt\n",
        "\n",
        "Done ( 0.0008780956268310547 seconds )\n",
        "\n",
        "=================================================================\n",
        "Testing indices and search on ./collections/urls.txt\n",
        "Building index with <class 'whooshmethods.WhooshBuilder'>\n",
        "Collection: ./collections/urls.txt\n",
        "Done ( 6.683624267578125 seconds )\n",
        "\n",
        "Reading index with <class 'whooshmethods.WhooshIndex'>\n",
        "Collection size: 3\n",
        "Vocabulary size: 5873\n",
        "  Top 5 most frequent terms:\n",
        "        entropy 392.0=392.0\n",
        "        bias    196.0=196.0\n",
        "        system  156.0=156.0\n",
        "        displaystyle    130.0=130.0\n",
        "        edit    107.0=107.0\n",
        "\n",
        "\n",
        "  Frequency of word \"wikipedia\" in document 0 - https://en.wikipedia.org/wiki/Simpson's_paradox\n",
        ": 5\n",
        "  Total frequency of word \"wikipedia\" in the collection: 21.0 occurrences over 3 documents\n",
        "  Docs containing the word'wikipedia': 3\n",
        "Done ( 0.0902857780456543 seconds )\n",
        "\n",
        "------------------------------\n",
        "Checking search results\n",
        "  WhooshSearcher for query 'information probability'\n",
        "2.9275452380913167       https://en.wikipedia.org/wiki/Entropy\n",
        "2.475988540984066        https://en.wikipedia.org/wiki/Simpson's_paradox\n",
        "\n",
        "2.084641587814708        https://en.wikipedia.org/wiki/Bias\n",
        "\n",
        "\n",
        "Done ( 0.02002573013305664 seconds )\n",
        "\n",
        "  VSMDotProductSearcher for query 'information probability'\n",
        "2.201213664260036        https://en.wikipedia.org/wiki/Entropy\n",
        "1.155870467654375        https://en.wikipedia.org/wiki/Bias\n",
        "\n",
        "1.0252432526434019       https://en.wikipedia.org/wiki/Simpson's_paradox\n",
        "\n",
        "\n",
        "Done ( 0.0023403167724609375 seconds )\n",
        "\n",
        "  VSMCosineSearcher for query 'information probability'\n",
        "0.021229211008499366     https://en.wikipedia.org/wiki/Simpson's_paradox\n",
        "\n",
        "0.017476582465596094     https://en.wikipedia.org/wiki/Entropy\n",
        "0.00952397401539877      https://en.wikipedia.org/wiki/Bias\n",
        "\n",
        "\n",
        "Done ( 0.002106904983520508 seconds )\n",
        "\n",
        "=================================================================\n",
        "Testing indices and search on ./collections/docs1k.zip\n",
        "Building index with <class 'whooshmethods.WhooshBuilder'>\n",
        "Collection: ./collections/docs1k.zip\n",
        "Done ( 57.39169192314148 seconds )\n",
        "\n",
        "Reading index with <class 'whooshmethods.WhooshIndex'>\n",
        "Collection size: 998\n",
        "Vocabulary size: 118022\n",
        "  Top 5 most frequent terms:\n",
        "        family  175766.0=175766.0\n",
        "        tree    46705.0=46705.0\n",
        "        history 45744.0=45744.0\n",
        "        genealogy       45405.0=45405.0\n",
        "        surname 44965.0=44965.0\n",
        "\n",
        "\n",
        "  Frequency of word \"seat\" in document 0 - clueweb09-en0000-01-22977.html: 28\n",
        "  Total frequency of word \"seat\" in the collection: 1392.0 occurrences over 119 documents\n",
        "  Docs containing the word'seat': 119\n",
        "Done ( 1.921018123626709 seconds )\n",
        "\n",
        "------------------------------\n",
        "Checking search results\n",
        "  WhooshSearcher for query 'obama family tree'\n",
        "16.510355036382904       clueweb09-en0010-79-2218.html\n",
        "15.912263448137267       clueweb09-en0010-57-32937.html\n",
        "15.833557824663444       clueweb09-en0001-02-21241.html\n",
        "15.635409879945282       clueweb09-en0008-45-29117.html\n",
        "15.578551656409365       clueweb09-enwp01-59-16163.html\n",
        "15.578551656409365       clueweb09-enwp02-06-15081.html\n",
        "\n",
        "Done ( 0.10114669799804688 seconds )\n",
        "\n",
        "  VSMDotProductSearcher for query 'obama family tree'\n",
        "49.73029577380436        clueweb09-enwp01-59-16163.html\n",
        "49.73029577380436        clueweb09-enwp02-06-15081.html\n",
        "49.71745330976961        clueweb09-enwp03-00-6901.html\n",
        "49.71745330976961        clueweb09-enwp03-07-2998.html\n",
        "49.69823464015178        clueweb09-enwp00-00-9498.html\n",
        "\n",
        "Done ( 0.8824598789215088 seconds )\n",
        "\n",
        "  VSMCosineSearcher for query 'obama family tree'\n",
        "0.28480133969659016      clueweb09-en0010-79-2218.html\n",
        "0.22630799249136074      clueweb09-en0009-30-2768.html\n",
        "0.22535524448825275      clueweb09-en0001-02-21241.html\n",
        "0.22386434086185886      clueweb09-en0009-30-2441.html\n",
        "0.2234923487378765       clueweb09-en0009-30-2755.html\n",
        "\n",
        "Done ( 0.8222432136535645 seconds )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Enunciado P1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
